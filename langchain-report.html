<!DOCTYPE html>
<html lang="en" data-theme="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>aisentry Report</title>
    <style>
        
        /* CSS Variables for theming */
        :root, [data-theme="light"] {
            --bg-primary: #f8fafc;
            --bg-secondary: #ffffff;
            --bg-tertiary: #f1f5f9;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --text-muted: #94a3b8;
            --border-color: #e2e8f0;
            --accent-primary: #f97316;
            --accent-secondary: #ea580c;
            --accent-gradient: linear-gradient(135deg, #f97316 0%, #ea580c 100%);
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --info: #3b82f6;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --card-shadow-hover: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }

        [data-theme="dark"] {
            --bg-primary: #0f172a;
            --bg-secondary: #1e293b;
            --bg-tertiary: #334155;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #64748b;
            --border-color: #334155;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.3);
            --card-shadow-hover: 0 10px 15px -3px rgba(0, 0, 0, 0.4);
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            transition: background 0.3s, color 0.3s;
        }

        .container { max-width: 1280px; margin: 0 auto; padding: 24px; }

        /* Header */
        header {
            background: var(--accent-gradient);
            color: white;
            padding: 24px 32px;
            border-radius: 16px;
            margin-bottom: 24px;
            box-shadow: 0 10px 40px rgba(249, 115, 22, 0.3);
        }
        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .header-left {
            display: flex;
            align-items: center;
            gap: 16px;
        }
        .logo {
            background: rgba(255,255,255,0.2);
            padding: 8px;
            border-radius: 12px;
        }
        header h1 { font-size: 1.5em; font-weight: 700; margin-bottom: 2px; }
        .generated { opacity: 0.9; font-size: 0.85em; }

        /* Theme Toggle */
        .theme-toggle {
            background: rgba(255,255,255,0.2);
            border: none;
            padding: 10px;
            border-radius: 10px;
            cursor: pointer;
            color: white;
            transition: all 0.3s;
        }
        .theme-toggle:hover {
            background: rgba(255,255,255,0.3);
            transform: scale(1.05);
        }
        [data-theme="light"] .moon-icon { display: none; }
        [data-theme="dark"] .sun-icon { display: none; }

        /* Cards */
        .summary-card {
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 24px;
            margin-bottom: 24px;
            box-shadow: var(--card-shadow);
            transition: box-shadow 0.3s, transform 0.3s;
        }
        .summary-card:hover {
            box-shadow: var(--card-shadow-hover);
        }
        .summary-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
        }
        .metric {
            text-align: center;
            padding: 20px;
            background: var(--bg-tertiary);
            border-radius: 12px;
            transition: transform 0.3s;
        }
        .metric:hover { transform: translateY(-2px); }
        .metric-value {
            font-size: 2.5em;
            font-weight: 800;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .metric-label { color: var(--text-secondary); font-size: 0.9em; margin-top: 4px; }

        /* Circular Score Gauge */
        .score-gauge {
            position: relative;
            width: 120px;
            height: 120px;
            margin: 0 auto 12px;
        }
        .score-gauge svg {
            transform: rotate(-90deg);
        }
        .score-gauge-bg {
            fill: none;
            stroke: var(--bg-tertiary);
            stroke-width: 8;
        }
        .score-gauge-fill {
            fill: none;
            stroke-width: 8;
            stroke-linecap: round;
            transition: stroke-dashoffset 1s ease-out;
        }
        .score-gauge-text {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.8em;
            font-weight: 800;
        }
        .score-high .score-gauge-fill { stroke: var(--success); }
        .score-medium .score-gauge-fill { stroke: var(--warning); }
        .score-low .score-gauge-fill { stroke: var(--danger); }
        .score-high .score-gauge-text { color: var(--success); }
        .score-medium .score-gauge-text { color: var(--warning); }
        .score-low .score-gauge-text { color: var(--danger); }

        .section { margin-bottom: 32px; }
        .section h2 {
            color: var(--text-primary);
            font-size: 1.3em;
            font-weight: 700;
            padding-bottom: 12px;
            margin-bottom: 16px;
            border-bottom: 3px solid var(--accent-primary);
            display: inline-block;
        }
        .section h3 {
            color: var(--text-primary);
            font-size: 1.1em;
            font-weight: 600;
            margin-bottom: 16px;
        }

        /* Findings & Vulnerabilities */
        .finding, .vulnerability {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 12px;
            box-shadow: var(--card-shadow);
            border-left: 4px solid;
            transition: all 0.3s;
        }
        .finding:hover, .vulnerability:hover {
            box-shadow: var(--card-shadow-hover);
            transform: translateX(4px);
        }
        .severity-critical { border-left-color: #ef4444; }
        .severity-high { border-left-color: #f97316; }
        .severity-medium { border-left-color: #f59e0b; }
        .severity-low { border-left-color: #3b82f6; }
        .severity-info { border-left-color: #64748b; }

        .finding-header, .vuln-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 12px;
            margin-bottom: 12px;
            flex-wrap: wrap;
        }
        .finding-title, .vuln-title {
            font-weight: 600;
            font-size: 1.05em;
            color: var(--text-primary);
        }

        /* Badges with icons */
        .badge {
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.75em;
            font-weight: 600;
            text-transform: uppercase;
            color: white;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        .badge::before { font-size: 0.9em; }
        .badge-critical { background: linear-gradient(135deg, #ef4444, #dc2626); }
        .badge-critical::before { content: "üî¥"; }
        .badge-high { background: linear-gradient(135deg, #f97316, #ea580c); }
        .badge-high::before { content: "üü†"; }
        .badge-medium { background: linear-gradient(135deg, #f59e0b, #d97706); color: #1e293b; }
        .badge-medium::before { content: "üü°"; }
        .badge-low { background: linear-gradient(135deg, #3b82f6, #2563eb); }
        .badge-low::before { content: "üîµ"; }
        .badge-info { background: linear-gradient(135deg, #64748b, #475569); }
        .badge-info::before { content: "‚ö™"; }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 16px;
            border-radius: 10px;
            overflow-x: auto;
            font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', monospace;
            font-size: 0.85em;
            margin: 12px 0;
            border: 1px solid var(--border-color);
        }
        .location {
            color: var(--text-muted);
            font-size: 0.85em;
            margin-bottom: 8px;
            font-family: monospace;
        }
        .description {
            margin: 12px 0;
            color: var(--text-secondary);
            line-height: 1.7;
        }
        .remediation {
            background: linear-gradient(135deg, rgba(249, 115, 22, 0.1), rgba(234, 88, 12, 0.05));
            border: 1px solid rgba(249, 115, 22, 0.2);
            padding: 16px;
            border-radius: 10px;
            margin-top: 12px;
        }
        [data-theme="dark"] .remediation {
            background: linear-gradient(135deg, rgba(249, 115, 22, 0.15), rgba(234, 88, 12, 0.1));
        }
        .remediation-title {
            font-weight: 600;
            color: var(--accent-primary);
            margin-bottom: 6px;
            display: flex;
            align-items: center;
            gap: 6px;
        }
        .remediation-title::before { content: "üí°"; }

        .detector-card {
            background: var(--bg-secondary);
            border-radius: 12px;
            padding: 16px 20px;
            margin-bottom: 12px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: var(--card-shadow);
            transition: all 0.3s;
        }
        .detector-card:hover {
            box-shadow: var(--card-shadow-hover);
            transform: translateY(-2px);
        }
        .detector-info h3 { font-size: 1em; margin-bottom: 4px; color: var(--text-primary); }
        .detector-stats { display: flex; gap: 24px; }
        .stat { text-align: center; }
        .stat-value { font-weight: 700; font-size: 1.3em; color: var(--accent-primary); }
        .stat-label { font-size: 0.75em; color: var(--text-muted); text-transform: uppercase; letter-spacing: 0.5px; }

        /* Animated Progress Bar */
        .progress-bar {
            height: 8px;
            background: var(--bg-tertiary);
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }
        .progress-fill {
            height: 100%;
            background: var(--accent-gradient);
            border-radius: 4px;
            animation: progressFill 1s ease-out forwards;
            transform-origin: left;
        }
        @keyframes progressFill {
            from { transform: scaleX(0); }
            to { transform: scaleX(1); }
        }

        footer {
            text-align: center;
            padding: 24px;
            color: var(--text-muted);
            font-size: 0.9em;
            border-top: 1px solid var(--border-color);
            margin-top: 32px;
        }
        footer a {
            color: var(--accent-primary);
            text-decoration: none;
            font-weight: 500;
        }
        footer a:hover { text-decoration: underline; }

        .severity-breakdown {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            margin-top: 10px;
        }
        .severity-item {
            display: flex;
            align-items: center;
            gap: 5px;
            padding: 5px 10px;
            background: #f8f9fa;
            border-radius: 4px;
        }
        .severity-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
        }

        /* Filter Toolbar - Modern Sticky Design */
        .filter-toolbar {
            position: sticky;
            top: 0;
            z-index: 100;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 12px;
            padding: 16px 20px;
            margin-bottom: 20px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.15);
        }
        .filter-toolbar-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 12px;
        }
        .filter-toolbar-title {
            color: white;
            font-size: 0.9em;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 8px;
        }
        .filter-toolbar-title svg {
            width: 18px;
            height: 18px;
        }
        .filter-stats-badge {
            background: rgba(255,255,255,0.15);
            color: white;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85em;
        }
        .filter-stats-badge strong {
            color: #60a5fa;
        }
        .filter-sections {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            align-items: flex-start;
        }
        .filter-section {
            flex: 1;
            min-width: 180px;
        }
        .filter-section-label {
            color: rgba(255,255,255,0.7);
            font-size: 0.75em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        .filter-section-actions {
            font-size: 0.9em;
        }
        .filter-section-actions a {
            color: #60a5fa;
            text-decoration: none;
            margin-left: 8px;
        }
        .filter-section-actions a:hover {
            text-decoration: underline;
        }
        .filter-chips {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
        }
        .filter-chip {
            display: inline-flex;
            align-items: center;
            gap: 4px;
            padding: 6px 12px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
            border: none;
            user-select: none;
        }
        .filter-chip.severity-chip {
            background: rgba(255,255,255,0.1);
            color: rgba(255,255,255,0.6);
        }
        .filter-chip.severity-chip.active[data-value="CRITICAL"] {
            background: #dc3545;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="HIGH"] {
            background: #fd7e14;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="MEDIUM"] {
            background: #ffc107;
            color: #333;
        }
        .filter-chip.severity-chip.active[data-value="LOW"] {
            background: #17a2b8;
            color: white;
        }
        .filter-chip.severity-chip.active[data-value="INFO"] {
            background: #6c757d;
            color: white;
        }
        .filter-chip.category-chip {
            background: rgba(255,255,255,0.1);
            color: rgba(255,255,255,0.6);
        }
        .filter-chip.category-chip.active {
            background: #667eea;
            color: white;
        }
        .filter-chip:hover {
            transform: translateY(-1px);
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }
        .filter-chip .chip-count {
            font-size: 0.85em;
            opacity: 0.8;
        }
        .filter-search-box {
            flex: 1;
            min-width: 200px;
        }
        .filter-search-input {
            width: 100%;
            padding: 10px 14px;
            border: 2px solid rgba(255,255,255,0.1);
            border-radius: 8px;
            background: rgba(255,255,255,0.05);
            color: white;
            font-size: 0.9em;
            transition: all 0.2s;
        }
        .filter-search-input::placeholder {
            color: rgba(255,255,255,0.4);
        }
        .filter-search-input:focus {
            outline: none;
            border-color: #667eea;
            background: rgba(255,255,255,0.1);
        }
        .filter-reset-btn {
            background: rgba(255,255,255,0.1);
            border: none;
            color: white;
            padding: 8px 16px;
            border-radius: 6px;
            font-size: 0.85em;
            cursor: pointer;
            transition: all 0.2s;
            white-space: nowrap;
        }
        .filter-reset-btn:hover {
            background: rgba(255,255,255,0.2);
        }

        /* Hidden class for filtered items */
        .finding.filtered-out,
        .vulnerability.filtered-out {
            display: none !important;
        }

        /* No results message */
        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: #666;
            background: white;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        }
        .no-results-icon {
            font-size: 3em;
            margin-bottom: 15px;
        }
        .no-results h3 {
            color: #333;
            margin-bottom: 8px;
        }

        /* Tab Navigation - Pill Style */
        .tab-navigation {
            display: flex;
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 6px;
            box-shadow: var(--card-shadow);
            margin-bottom: 0;
            gap: 6px;
        }
        .tab-btn {
            flex: 1;
            padding: 14px 24px;
            border: none;
            background: transparent;
            font-size: 0.95em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            border-radius: 12px;
            color: var(--text-secondary);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }
        .tab-btn::before {
            font-size: 1.1em;
        }
        .tab-btn[data-tab="vulnerabilities"]::before { content: "üõ°Ô∏è"; }
        .tab-btn[data-tab="security-posture"]::before { content: "üìä"; }
        .tab-btn:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }
        .tab-btn.active {
            background: var(--accent-gradient);
            color: white;
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .tab-btn .tab-badge {
            background: var(--bg-tertiary);
            color: var(--text-secondary);
            padding: 3px 10px;
            border-radius: 20px;
            font-size: 0.8em;
            font-weight: 600;
        }
        .tab-btn.active .tab-badge {
            background: rgba(255,255,255,0.25);
            color: white;
        }
        .tab-content {
            display: none;
            background: var(--bg-secondary);
            border-radius: 16px;
            padding: 24px;
            box-shadow: var(--card-shadow);
            margin-top: 12px;
            margin-bottom: 24px;
            animation: fadeIn 0.3s ease;
        }
        .tab-content.active {
            display: block;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Audit-specific styles */
        .audit-summary {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
            gap: 16px;
            margin-bottom: 24px;
        }
        .audit-metric {
            text-align: center;
            padding: 24px 20px;
            background: var(--bg-tertiary);
            border-radius: 16px;
            transition: all 0.3s;
            border: 1px solid var(--border-color);
        }
        .audit-metric:hover {
            transform: translateY(-4px);
            box-shadow: var(--card-shadow);
        }
        .audit-metric-value {
            font-size: 2.5em;
            font-weight: 800;
            background: var(--accent-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        .audit-metric-label {
            color: var(--text-secondary);
            font-size: 0.85em;
            margin-top: 6px;
            font-weight: 500;
        }
        .maturity-badge {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
            margin-top: 10px;
        }
        .maturity-badge::before { font-size: 0.9em; }
        .maturity-initial { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .maturity-initial::before { content: "‚ö†Ô∏è"; }
        .maturity-developing { background: linear-gradient(135deg, #ffedd5, #fed7aa); color: #ea580c; }
        .maturity-developing::before { content: "üîÑ"; }
        .maturity-defined { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .maturity-defined::before { content: "üìã"; }
        .maturity-managed { background: linear-gradient(135deg, #d1fae5, #a7f3d0); color: #059669; }
        .maturity-managed::before { content: "‚úÖ"; }
        .maturity-optimizing { background: linear-gradient(135deg, #cffafe, #a5f3fc); color: #0891b2; }
        .maturity-optimizing::before { content: "üöÄ"; }

        .category-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .category-card {
            background: var(--bg-tertiary);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            transition: all 0.3s;
            overflow: hidden;
        }
        .category-card:hover {
            box-shadow: var(--card-shadow);
        }
        .category-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px;
            cursor: pointer;
            user-select: none;
            transition: background 0.2s;
        }
        .category-header:hover {
            background: var(--bg-secondary);
        }
        .category-header-left {
            display: flex;
            align-items: center;
            gap: 12px;
        }
        .category-name {
            font-weight: 600;
            color: var(--text-primary);
            font-size: 1.05em;
        }
        .category-score {
            font-weight: 800;
            color: var(--accent-primary);
            font-size: 1.2em;
        }
        .accordion-icon {
            width: 20px;
            height: 20px;
            transition: transform 0.3s ease;
            color: var(--text-muted);
        }
        .category-card.open .accordion-icon {
            transform: rotate(180deg);
        }
        .category-progress {
            height: 4px;
            background: var(--border-color);
            overflow: hidden;
        }
        .category-progress-fill {
            height: 100%;
            background: var(--accent-gradient);
            animation: progressFill 1s ease-out forwards;
            transform-origin: left;
        }
        .category-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
        }
        .category-card.open .category-content {
            max-height: 2000px;
            transition: max-height 0.5s ease-in;
        }
        .control-list {
            list-style: none;
            padding: 0 20px 20px;
        }
        .control-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 10px 0;
            border-bottom: 1px solid var(--border-color);
        }
        .control-item:last-child {
            border-bottom: none;
        }
        .control-name {
            font-size: 0.9em;
            color: var(--text-secondary);
        }
        .control-status {
            font-size: 0.75em;
            padding: 4px 12px;
            border-radius: 20px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        .control-status::before { font-size: 0.85em; }
        .status-detected { background: linear-gradient(135deg, #d1fae5, #a7f3d0); color: #059669; }
        .status-detected::before { content: "‚úì"; }
        .status-missing { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .status-missing::before { content: "‚úó"; }
        .status-partial { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .status-partial::before { content: "~"; }
        .category-stats {
            display: flex;
            gap: 16px;
            padding: 12px 20px;
            background: var(--bg-secondary);
            border-top: 1px solid var(--border-color);
            font-size: 0.75rem;
            color: var(--text-muted);
        }
        .stat-item {
            display: flex;
            align-items: center;
            gap: 4px;
        }
        .stat-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
        }
        .stat-dot.detected { background: #059669; }
        .stat-dot.partial { background: #d97706; }
        .stat-dot.missing { background: #dc2626; }
        .section-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 20px 0 16px;
        }
        .section-header h3 {
            margin: 0;
            color: var(--text-primary);
            border-bottom: 2px solid var(--accent-primary);
            padding-bottom: 8px;
        }
        .expand-toggle {
            font-size: 0.8rem;
            color: var(--accent-primary);
            cursor: pointer;
            padding: 4px 12px;
            border: 1px solid var(--accent-primary);
            border-radius: 6px;
            background: var(--bg-secondary);
            transition: all 0.2s;
        }
        .expand-toggle:hover {
            background: var(--accent-primary);
            color: white;
        }

        .recommendations-section {
            margin-top: 32px;
        }
        .recommendations-section h3 {
            color: var(--text-primary);
            font-size: 1.15em;
            font-weight: 700;
            margin-bottom: 16px;
            padding-bottom: 10px;
            border-bottom: 3px solid var(--accent-primary);
            display: inline-block;
        }
        .rec-list {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }
        .rec-item {
            background: var(--bg-tertiary);
            border-radius: 12px;
            padding: 18px;
            border-left: 4px solid;
            transition: all 0.3s;
        }
        .rec-item:hover {
            transform: translateX(4px);
            box-shadow: var(--card-shadow);
        }
        .rec-critical { border-color: #ef4444; }
        .rec-high { border-color: #f97316; }
        .rec-medium { border-color: #f59e0b; }
        .rec-low { border-color: #3b82f6; }
        .rec-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 12px;
            margin-bottom: 10px;
            flex-wrap: wrap;
        }
        .rec-title {
            font-weight: 600;
            color: var(--text-primary);
            line-height: 1.4;
        }
        .rec-priority {
            font-size: 0.7em;
            padding: 4px 10px;
            border-radius: 20px;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .priority-critical { background: linear-gradient(135deg, #fee2e2, #fecaca); color: #dc2626; }
        .priority-high { background: linear-gradient(135deg, #ffedd5, #fed7aa); color: #ea580c; }
        .priority-medium { background: linear-gradient(135deg, #fef3c7, #fde68a); color: #d97706; }
        .priority-low { background: linear-gradient(135deg, #dbeafe, #bfdbfe); color: #2563eb; }
        .rec-description {
            color: var(--text-secondary);
            font-size: 0.9em;
            line-height: 1.6;
        }

        /* Combined score display */
        .combined-score-section {
            background: var(--accent-gradient);
            border-radius: 20px;
            padding: 28px;
            margin-bottom: 24px;
            color: white;
            box-shadow: 0 10px 40px rgba(249, 115, 22, 0.3);
        }
        .combined-score-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
            gap: 20px;
            text-align: center;
        }
        .combined-metric {
            padding: 20px;
            background: rgba(255,255,255,0.15);
            border-radius: 16px;
            backdrop-filter: blur(10px);
            transition: all 0.3s;
        }
        .combined-metric:hover {
            background: rgba(255,255,255,0.25);
            transform: translateY(-4px);
        }
        .combined-metric-value {
            font-size: 2.8em;
            font-weight: 800;
        }
        .combined-metric-label {
            font-size: 0.85em;
            opacity: 0.95;
            margin-top: 6px;
            font-weight: 500;
        }

        /* Severity Filter Buttons */
        .severity-filter-btn {
            padding: 10px 18px;
            border: 2px solid var(--border-color);
            border-radius: 25px;
            background: var(--bg-secondary);
            color: var(--text-secondary);
            font-size: 0.9em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 6px;
        }
        .severity-filter-btn::before {
            font-size: 0.9em;
        }
        .severity-filter-btn[data-severity="all"]::before { content: "üìã"; }
        .severity-filter-btn[data-severity="critical"]::before { content: "üî¥"; }
        .severity-filter-btn[data-severity="high"]::before { content: "üü†"; }
        .severity-filter-btn[data-severity="medium"]::before { content: "üü°"; }
        .severity-filter-btn[data-severity="low"]::before { content: "üîµ"; }
        .severity-filter-btn[data-severity="info"]::before { content: "‚ö™"; }
        .severity-filter-btn:hover {
            border-color: var(--accent-primary);
            color: var(--accent-primary);
            transform: translateY(-2px);
        }
        .severity-filter-btn.active {
            background: var(--accent-gradient);
            border-color: transparent;
            color: white;
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .severity-filter-btn[data-severity="critical"].active {
            background: linear-gradient(135deg, #ef4444, #dc2626);
            box-shadow: 0 4px 12px rgba(239, 68, 68, 0.3);
        }
        .severity-filter-btn[data-severity="high"].active {
            background: linear-gradient(135deg, #f97316, #ea580c);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3);
        }
        .severity-filter-btn[data-severity="medium"].active {
            background: linear-gradient(135deg, #f59e0b, #d97706);
            color: #1e293b;
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.3);
        }
        .severity-filter-btn[data-severity="low"].active {
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
        }
        .severity-filter-btn[data-severity="info"].active {
            background: linear-gradient(135deg, #64748b, #475569);
            box-shadow: 0 4px 12px rgba(100, 116, 139, 0.3);
        }
        .filter-count {
            opacity: 0.85;
            font-weight: 500;
        }

        /* Hidden items for pagination */
        .rec-hidden, .finding-hidden {
            display: none !important;
        }
        .rec-filtered, .finding-filtered {
            display: none !important;
        }

        /* Load More Button */
        .load-more-btn {
            padding: 14px 36px;
            background: var(--accent-gradient);
            border: none;
            border-radius: 30px;
            color: white;
            font-size: 1em;
            font-weight: 700;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 6px 20px rgba(249, 115, 22, 0.35);
            display: inline-flex;
            align-items: center;
            gap: 8px;
        }
        .load-more-btn::before {
            content: "‚Üì";
            font-size: 1.1em;
        }
        .load-more-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(249, 115, 22, 0.45);
        }
        .load-more-btn:active {
            transform: translateY(-1px);
        }
        .load-more-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }
        #remaining-count, #findings-remaining-count {
            opacity: 0.9;
            font-weight: 500;
        }
        
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="header-left">
                    <div class="logo">
                        <svg width="40" height="40" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <path d="M12 22s8-4 8-10V5l-8-3-8 3v7c0 6 8 10 8 10z"/>
                            <path d="M9 12l2 2 4-4"/>
                        </svg>
                    </div>
                    <div>
                        <h1>aisentry Report</h1>
                        <p class="generated">Generated: 2026-01-10 18:54:38 UTC</p>
                    </div>
                </div>
                <button class="theme-toggle" onclick="toggleTheme()" title="Toggle dark mode">
                    <svg class="sun-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"/>
                        <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
                    </svg>
                    <svg class="moon-icon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                    </svg>
                </button>
            </div>
        </header>
        
        <div class="combined-score-section">
            <div class="combined-score-grid">
                <div class="combined-metric">
                    <div class="combined-metric-value">12</div>
                    <div class="combined-metric-label">Combined Security Score</div>
                </div>
                <div class="combined-metric">
                    <div class="combined-metric-value">5</div>
                    <div class="combined-metric-label">Vulnerability Score</div>
                </div>
        
                <div class="combined-metric">
                    <div class="combined-metric-value">19</div>
                    <div class="combined-metric-label">Security Posture</div>
                </div>
            
            </div>
        </div>
        
        <div class="tab-navigation">
            <button class="tab-btn active" data-tab="vulnerabilities">
                Vulnerabilities
                <span class="tab-badge">324</span>
            </button>
        
            <button class="tab-btn" data-tab="security-posture">
                Security Posture
                <span class="tab-badge">25/61</span>
            </button>
            </div>
        <div id="vulnerabilities" class="tab-content active">
            
        <div class="audit-summary">
            <div class="audit-metric">
                <div class="audit-metric-value">2503</div>
                <div class="audit-metric-label">Files Scanned</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">324</div>
                <div class="audit-metric-label">Issues Found</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">62%</div>
                <div class="audit-metric-label">Confidence</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">14.8s</div>
                <div class="audit-metric-label">Scan Time</div>
            </div>
        </div>
        
            <div class="section">
                <h3>Vulnerabilities (324)</h3>

                <!-- Severity Filter -->
                <div class="severity-filter" style="margin-bottom: 16px; display: flex; gap: 8px; flex-wrap: wrap;">
                    <button class="severity-filter-btn active" data-severity="all" data-target="findings" onclick="filterFindingsBySeverity('all')">
                        All <span class="filter-count">(324)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="critical" data-target="findings" onclick="filterFindingsBySeverity('critical')">
                        Critical <span class="filter-count">(135)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="high" data-target="findings" onclick="filterFindingsBySeverity('high')">
                        High <span class="filter-count">(134)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="medium" data-target="findings" onclick="filterFindingsBySeverity('medium')">
                        Medium <span class="filter-count">(23)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="low" data-target="findings" onclick="filterFindingsBySeverity('low')">
                        Low <span class="filter-count">(0)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="info" data-target="findings" onclick="filterFindingsBySeverity('info')">
                        Info <span class="filter-count">(32)</span>
                    </button>
                </div>

                <div class="findings-list" id="findings-list">
            
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="0">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/embeddings/base.py:429</div>
                    <div class="description">Function &#x27;_tokenize&#x27; on line 429 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _tokenize(
        self, texts: list[str], chunk_size: int
    ) -&gt; tuple[Iterable[int], list[list[int] | str], list[int], list[int]]:
        &quot;&quot;&quot;Tokenize and batch input texts.

        Splits texts based on `embedding_ctx_length` and groups them into batches
        of size `chunk_size`.

        Args:
            texts: The list of texts to tokenize.
            chunk_size: The maximum number of texts to include in a single batch.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium " data-severity="medium" data-category="LLM08: Excessive Agency" data-index="1">
                    <div class="finding-header">
                        <span class="finding-title">High-risk network operation without confirmation in &#x27;_tokenize&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/embeddings/base.py:429</div>
                    <div class="description">Function &#x27;_tokenize&#x27; on line 429 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            raise ValueError(msg)

    def _tokenize(
        self, texts: list[str], chunk_size: int
    ) -&gt; tuple[Iterable[int], list[list[int] | str], list[int], list[int]]:
        &quot;&quot;&quot;Tokenize and batch input texts.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="2">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:3754</div>
                    <div class="description">Function &#x27;_construct_responses_api_payload&#x27; on line 3754 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _construct_responses_api_payload(
    messages: Sequence[BaseMessage], payload: dict
) -&gt; dict:
    # Rename legacy parameters
    for legacy_token_param in [&quot;max_tokens&quot;, &quot;max_completion_tokens&quot;]:
        if legacy_token_param in payload:
            payload[&quot;max_output_tokens&quot;] = payload.pop(legacy_token_param)
    if &quot;reasoning_effort&quot; in payload and &quot;reasoning&quot; not in payload:
        payload[&quot;reasoning&quot;] = {&quot;effort&quot;: payload.pop(&quot;reasoning_effort&quot;)}

    # Remove temperature parameter for models that don&#x27;t support it in responses API</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="3">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1338</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 1338 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        self._ensure_sync_client_available()
        payload = self._get_request_payload(messages, stop=stop, **kwargs)
        generation_info = None
        raw_response = None</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high " data-severity="high" data-category="LLM04: Model Denial of Service" data-index="4">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1698</div>
                    <div class="description">Function &#x27;_get_encoding_model&#x27; on line 1698 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_encoding_model(self) -&gt; tuple[str, tiktoken.Encoding]:
        if self.tiktoken_model_name is not None:
            model = self.tiktoken_model_name
        else:
            model = self.model_name

        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            model_lower = model.lower()
            encoder = &quot;cl100k_base&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical " data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="5">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1714</div>
                    <div class="description">Function &#x27;get_token_ids&#x27; on line 1714 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_token_ids(self, text: str) -&gt; list[int]:
        &quot;&quot;&quot;Get the tokens present in the text with tiktoken package.&quot;&quot;&quot;
        if self.custom_get_token_ids is not None:
            return self.custom_get_token_ids(text)
        # tiktoken NOT supported for Python 3.7 or below
        if sys.version_info[1] &lt;= 7:
            return super().get_token_ids(text)
        _, encoding_model = self._get_encoding_model()
        return encoding_model.encode(text)

    def get_num_tokens_from_messages(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high " data-severity="high" data-category="LLM04: Model Denial of Service" data-index="6">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1724</div>
                    <div class="description">Function &#x27;get_num_tokens_from_messages&#x27; on line 1724 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def get_num_tokens_from_messages(
        self,
        messages: Sequence[BaseMessage],
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool] | None = None,
    ) -&gt; int:
        &quot;&quot;&quot;Calculate num tokens for `gpt-3.5-turbo` and `gpt-4` with `tiktoken` package.

        !!! warning
            You must have the `pillow` installed if you want to count image tokens if
            you are specifying the image as a base64 string, and you must have both
            `pillow` and `httpx` installed if you are specifying the image as a URL. If</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high " data-severity="high" data-category="LLM08: Excessive Agency" data-index="7">
                    <div class="finding-header">
                        <span class="finding-title">High-risk delete/write/network operation without confirmation in &#x27;_construct_responses_api_payload&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:3754</div>
                    <div class="description">Function &#x27;_construct_responses_api_payload&#x27; on line 3754 performs high-risk delete/write/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def _construct_responses_api_payload(
    messages: Sequence[BaseMessage], payload: dict
) -&gt; dict:
    # Rename legacy parameters</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high " data-severity="high" data-category="LLM08: Excessive Agency" data-index="8">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;_generate&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1338</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 1338 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            yield generation_chunk

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-medium " data-severity="medium" data-category="LLM08: Excessive Agency" data-index="9">
                    <div class="finding-header">
                        <span class="finding-title">High-risk network operation without confirmation in &#x27;get_num_tokens_from_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1724</div>
                    <div class="description">Function &#x27;get_num_tokens_from_messages&#x27; on line 1724 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return encoding_model.encode(text)

    def get_num_tokens_from_messages(
        self,
        messages: Sequence[BaseMessage],
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="10">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_construct_responses_api_payload&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:3754</div>
                    <div class="description">Function &#x27;_construct_responses_api_payload&#x27; on line 3754 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _construct_responses_api_payload(
    messages: Sequence[BaseMessage], payload: dict
) -&gt; dict:
    # Rename legacy parameters</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="11">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;get_num_tokens_from_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1724</div>
                    <div class="description">Function &#x27;get_num_tokens_from_messages&#x27; on line 1724 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return encoding_model.encode(text)

    def get_num_tokens_from_messages(
        self,
        messages: Sequence[BaseMessage],
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="12">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:833</div>
                    <div class="description">LLM output from &#x27;_convert_from_v1_to_ollama&#x27; is used in &#x27;UPDATE&#x27; on line 833 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                    update={
                        &quot;content&quot;: _convert_from_v1_to_ollama(
                            cast(&quot;list[types.ContentBlock]&quot;, message.content),
                            message.response_metadata.get(&quot;model_provider&quot;),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="13">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:720</div>
                    <div class="description">Function &#x27;_chat_params&#x27; on line 720 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _chat_params(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; dict[str, Any]:
        &quot;&quot;&quot;Assemble the parameters for a chat completion request.

        Args:
            messages: List of LangChain messages to send to the model.
            stop: Optional list of stop tokens to use for this invocation.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="14">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:812</div>
                    <div class="description">Function &#x27;_convert_messages_to_ollama_messages&#x27; on line 812 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _convert_messages_to_ollama_messages(
        self, messages: list[BaseMessage]
    ) -&gt; Sequence[Message]:
        &quot;&quot;&quot;Convert a BaseMessage list to list of messages for Ollama to consume.

        Args:
            messages: List of BaseMessage to convert.

        Returns:
            List of messages in Ollama format.
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="15">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:945</div>
                    <div class="description">Function &#x27;_create_chat_stream&#x27; on line 945 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_chat_stream(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[Mapping[str, Any] | str]:
        chat_params = self._chat_params(messages, stop, **kwargs)

        if chat_params[&quot;stream&quot;]:
            if self._client:
                yield from self._client.chat(**chat_params)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM05: Supply Chain Vulnerabilities" data-index="16">
                    <div class="finding-header">
                        <span class="finding-title">Code execution on external content</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:147</div>
                    <div class="description">eval() on non-literal content on line 147. </div>
                    <div class="code-block"><code>            # Use ast.literal_eval to safely parse Python-style dicts
            # (e.g. with single quotes)
            return ast.literal_eval(json_string)
        except (SyntaxError, ValueError) as e:
            # If both fail, and we&#x27;re not skipping, raise an informative error.
            if skip:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Code Execution:
1. NEVER use eval/exec on untrusted input
2. Use safe alternatives (json.loads, ast.literal_eval)
3. Validate and sanitize all external content
4. Use sandboxed execution environments
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="17">
                    <div class="finding-header">
                        <span class="finding-title">High-risk network operation without confirmation in &#x27;_chat_params&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:720</div>
                    <div class="description">Function &#x27;_chat_params&#x27; on line 720 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>    &quot;&quot;&quot;The async client to use for making requests.&quot;&quot;&quot;

    def _chat_params(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="18">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;_convert_messages_to_ollama_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:812</div>
                    <div class="description">Function &#x27;_convert_messages_to_ollama_messages&#x27; on line 812 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return self

    def _convert_messages_to_ollama_messages(
        self, messages: list[BaseMessage]
    ) -&gt; Sequence[Message]:
        &quot;&quot;&quot;Convert a BaseMessage list to list of messages for Ollama to consume.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="19">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write operation without confirmation in &#x27;_create_chat_stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:945</div>
                    <div class="description">Function &#x27;_create_chat_stream&#x27; on line 945 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            yield await self._async_client.chat(**chat_params)

    def _create_chat_stream(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="20">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_convert_messages_to_ollama_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:812</div>
                    <div class="description">Function &#x27;_convert_messages_to_ollama_messages&#x27; on line 812 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self

    def _convert_messages_to_ollama_messages(
        self, messages: list[BaseMessage]
    ) -&gt; Sequence[Message]:
        &quot;&quot;&quot;Convert a BaseMessage list to list of messages for Ollama to consume.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="21">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;_create_chat_stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:945</div>
                    <div class="description">Function &#x27;_create_chat_stream&#x27; on line 945 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            yield await self._async_client.chat(**chat_params)

    def _create_chat_stream(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="22">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/llms.py:356</div>
                    <div class="description">Function &#x27;_create_generate_stream&#x27; on line 356 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_generate_stream(
        self,
        prompt: str,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[Mapping[str, Any] | str]:
        if self._client:
            yield from self._client.generate(
                **self._generate_params(prompt, stop=stop, **kwargs)
            )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="23">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;_create_generate_stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/llms.py:356</div>
                    <div class="description">Function &#x27;_create_generate_stream&#x27; on line 356 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>                yield part

    def _create_generate_stream(
        self,
        prompt: str,
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="24">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;messages&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:752</div>
                    <div class="description">User input &#x27;messages&#x27; flows to LLM call via call in variable &#x27;llm_input&#x27;. Function &#x27;_generate&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>            return self._create_chat_result(answer)
        llm_input = self._to_chat_prompt(messages)

        if should_stream:
            stream_iter = self.llm._stream(
                llm_input, stop=stop, run_manager=run_manager, **kwargs</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="25">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:538</div>
                    <div class="description">Function &#x27;_inherit_llm_properties&#x27; on line 538 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _inherit_llm_properties(self) -&gt; None:
        &quot;&quot;&quot;Inherit properties from the wrapped LLM instance if not explicitly set.&quot;&quot;&quot;
        if not hasattr(self, &quot;llm&quot;) or self.llm is None:
            return

        # Map of ChatHuggingFace properties to LLM properties
        property_mappings = {
            &quot;temperature&quot;: &quot;temperature&quot;,
            &quot;max_tokens&quot;: &quot;max_new_tokens&quot;,  # Different naming convention
            &quot;top_p&quot;: &quot;top_p&quot;,
            &quot;seed&quot;: &quot;seed&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="26">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:723</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 723 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        stream: bool | None = None,  # noqa: FBT001
        **kwargs: Any,
    ) -&gt; ChatResult:
        should_stream = stream if stream is not None else self.streaming

        if _is_huggingface_textgen_inference(self.llm):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="27">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:410</div>
                    <div class="description">Function &#x27;_format_messages&#x27; on line 410 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _format_messages(
    messages: Sequence[BaseMessage],
) -&gt; tuple[str | list[dict] | None, list[dict]]:
    &quot;&quot;&quot;Format messages for Anthropic&#x27;s API.&quot;&quot;&quot;
    system: str | list[dict] | None = None
    formatted_messages: list[dict] = []
    merged_messages = _merge_messages(messages)
    for _i, message in enumerate(merged_messages):
        if message.type == &quot;system&quot;:
            if system is not None:
                msg = &quot;Received multiple non-consecutive system messages.&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="28">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1792</div>
                    <div class="description">Function &#x27;convert_to_anthropic_tool&#x27; on line 1792 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def convert_to_anthropic_tool(
    tool: Mapping[str, Any] | type | Callable | BaseTool,
    *,
    strict: bool | None = None,
) -&gt; AnthropicTool:
    &quot;&quot;&quot;Convert a tool-like object to an Anthropic tool definition.

    Args:
        tool: A tool-like object to convert. Can be an Anthropic tool dict,
            a Pydantic model, a function, or a `BaseTool`.
        strict: If `True`, enables strict schema adherence for the tool.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="29">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1881</div>
                    <div class="description">Function &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27; on line 1881 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _lc_tool_calls_to_anthropic_tool_use_blocks(
    tool_calls: list[ToolCall],
) -&gt; list[_AnthropicToolUse]:
    return [
        _AnthropicToolUse(
            type=&quot;tool_use&quot;,
            name=tool_call[&quot;name&quot;],
            input=tool_call[&quot;args&quot;],
            id=cast(&quot;str&quot;, tool_call[&quot;id&quot;]),
        )
        for tool_call in tool_calls</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="30">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1004</div>
                    <div class="description">Function &#x27;_client&#x27; on line 1004 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _client(self) -&gt; anthropic.Client:
        client_params = self._client_params
        http_client_params = {&quot;base_url&quot;: client_params[&quot;base_url&quot;]}
        if &quot;timeout&quot; in client_params:
            http_client_params[&quot;timeout&quot;] = client_params[&quot;timeout&quot;]
        if self.anthropic_proxy:
            http_client_params[&quot;anthropic_proxy&quot;] = self.anthropic_proxy
        http_client = _get_default_httpx_client(**http_client_params)
        params = {
            **client_params,
            &quot;http_client&quot;: http_client,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="31">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1019</div>
                    <div class="description">Function &#x27;_async_client&#x27; on line 1019 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _async_client(self) -&gt; anthropic.AsyncClient:
        client_params = self._client_params
        http_client_params = {&quot;base_url&quot;: client_params[&quot;base_url&quot;]}
        if &quot;timeout&quot; in client_params:
            http_client_params[&quot;timeout&quot;] = client_params[&quot;timeout&quot;]
        if self.anthropic_proxy:
            http_client_params[&quot;anthropic_proxy&quot;] = self.anthropic_proxy
        http_client = _get_default_async_httpx_client(**http_client_params)
        params = {
            **client_params,
            &quot;http_client&quot;: http_client,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="32">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1885</div>
                    <div class="description">Function &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27; on line 1881 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>) -&gt; list[_AnthropicToolUse]:
    return [
        _AnthropicToolUse(
            type=&quot;tool_use&quot;,
            name=tool_call[&quot;name&quot;],
            input=tool_call[&quot;args&quot;],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="33">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;_format_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:489</div>
                    <div class="description">Function &#x27;_format_messages&#x27; on line 410 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>                            else:
                                args = {}
                            tool_use_block = _AnthropicToolUse(
                                type=&quot;tool_use&quot;,
                                name=block[&quot;name&quot;],
                                input=args,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="34">
                    <div class="finding-header">
                        <span class="finding-title">High-risk delete/execute/network operation without confirmation in &#x27;_format_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:410</div>
                    <div class="description">Function &#x27;_format_messages&#x27; on line 410 performs high-risk delete/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def _format_messages(
    messages: Sequence[BaseMessage],
) -&gt; tuple[str | list[dict] | None, list[dict]]:
    &quot;&quot;&quot;Format messages for Anthropic&#x27;s API.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="35">
                    <div class="finding-header">
                        <span class="finding-title">High-risk network operation without confirmation in &#x27;convert_to_anthropic_tool&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1792</div>
                    <div class="description">Function &#x27;convert_to_anthropic_tool&#x27; on line 1792 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def convert_to_anthropic_tool(
    tool: Mapping[str, Any] | type | Callable | BaseTool,
    *,
    strict: bool | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="36">
                    <div class="finding-header">
                        <span class="finding-title">High-risk network operation without confirmation in &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1881</div>
                    <div class="description">Function &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27; on line 1881 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def _lc_tool_calls_to_anthropic_tool_use_blocks(
    tool_calls: list[ToolCall],
) -&gt; list[_AnthropicToolUse]:
    return [</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="37">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_format_messages&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:410</div>
                    <div class="description">Function &#x27;_format_messages&#x27; on line 410 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _format_messages(
    messages: Sequence[BaseMessage],
) -&gt; tuple[str | list[dict] | None, list[dict]]:
    &quot;&quot;&quot;Format messages for Anthropic&#x27;s API.&quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="38">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/qdrant/langchain_qdrant/fastembed_sparse.py:70</div>
                    <div class="description">Function &#x27;embed_documents&#x27; on line 70 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def embed_documents(self, texts: list[str]) -&gt; list[SparseVector]:
        results = self._model.embed(
            texts, batch_size=self._batch_size, parallel=self._parallel
        )
        return [
            SparseVector(indices=result.indices.tolist(), values=result.values.tolist())
            for result in results
        ]

    def embed_query(self, text: str) -&gt; SparseVector:
        result = next(self._model.query_embed(text))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="39">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/qdrant/langchain_qdrant/fastembed_sparse.py:79</div>
                    <div class="description">Function &#x27;embed_query&#x27; on line 79 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def embed_query(self, text: str) -&gt; SparseVector:
        result = next(self._model.query_embed(text))

        return SparseVector(
            indices=result.indices.tolist(), values=result.values.tolist()
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="40">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:415</div>
                    <div class="description">Function &#x27;_stream&#x27; on line 415 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _stream(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[ChatGenerationChunk]:
        message_dicts, params = self._create_message_dicts(messages, stop)
        params = {**params, **kwargs}
        default_chunk_class = AIMessageChunk
        params.pop(&quot;stream&quot;, None)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="41">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:589</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 589 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -&gt; ChatResult:
        if self.streaming:
            stream_iter = self._stream(
                messages, stop=stop, run_manager=run_manager, **kwargs
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="42">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute operation without confirmation in &#x27;_stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:415</div>
                    <div class="description">Function &#x27;_stream&#x27; on line 415 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            return default_class(content=content)  # type: ignore[call-arg]

    def _stream(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="43">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute operation without confirmation in &#x27;_generate&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:589</div>
                    <div class="description">Function &#x27;_generate&#x27; on line 589 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            yield chunk

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="44">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:422</div>
                    <div class="description">LLM output from &#x27;beta_model.bind_tools&#x27; is used in &#x27;UPDATE&#x27; on line 422 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            beta_model = self.model_copy(update={&quot;api_base&quot;: DEFAULT_BETA_API_BASE})
            return beta_model.bind_tools(
                tools,
                tool_choice=tool_choice,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="45">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:528</div>
                    <div class="description">LLM output from &#x27;beta_model.with_structured_output&#x27; is used in &#x27;UPDATE&#x27; on line 528 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            beta_model = self.model_copy(update={&quot;api_base&quot;: DEFAULT_BETA_API_BASE})
            return beta_model.with_structured_output(
                schema,
                method=method,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="46">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</div>
                    <div class="description">Function &#x27;bind_tools&#x27; on line 395 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def bind_tools(
        self,
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool],
        *,
        tool_choice: dict | str | bool | None = None,
        strict: bool | None = None,
        parallel_tool_calls: bool | None = None,
        **kwargs: Any,
    ) -&gt; Runnable[LanguageModelInput, AIMessage]:
        &quot;&quot;&quot;Bind tool-like objects to this chat model.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="47">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;bind_tools&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:422</div>
                    <div class="description">Function &#x27;bind_tools&#x27; on line 395 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>            # Create a new instance with beta endpoint
            beta_model = self.model_copy(update={&quot;api_base&quot;: DEFAULT_BETA_API_BASE})
            return beta_model.bind_tools(
                tools,
                tool_choice=tool_choice,
                strict=strict,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="48">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;bind_tools&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</div>
                    <div class="description">Function &#x27;bind_tools&#x27; on line 395 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            ) from e

    def bind_tools(
        self,
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool],
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="49">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;bind_tools&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</div>
                    <div class="description">Function &#x27;bind_tools&#x27; on line 395 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>            ) from e

    def bind_tools(
        self,
        tools: Sequence[dict[str, Any] | type | Callable | BaseTool],
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM05: Supply Chain Vulnerabilities" data-index="50">
                    <div class="finding-header">
                        <span class="finding-title">Network fetch combined with code execution</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/groq/langchain_groq/chat_models.py:1383</div>
                    <div class="description">This file downloads external content (lines [441]) and executes code (lines [1383, 1384, 1385]). This pattern enables remote code execution attacks if the fetched content is not properly validated.</div>
                    <div class="code-block"><code>        return HumanMessageChunk(content=content)
    if role == &quot;assistant&quot; or default_class == AIMessageChunk:
        if reasoning := _dict.get(&quot;reasoning&quot;):
            additional_kwargs[&quot;reasoning_content&quot;] = reasoning
        if executed_tools := _dict.get(&quot;executed_tools&quot;):
            additional_kwargs[&quot;executed_tools&quot;] = []
            for executed_tool in executed_tools:
                if executed_tool.get(&quot;output&quot;):
                    # Tool output duplicates query and other server tool call data
                    additional_kwargs[&quot;executed_tools&quot;].append(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="51">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/prompty/langchain_prompty/core.py:191</div>
                    <div class="description">Function &#x27;__call__&#x27; on line 191 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __call__(self, data: BaseModel) -&gt; BaseModel:
        return self.invoke(data)


class NoOpParser(Invoker):
    &quot;&quot;&quot;NoOp parser for invokers.&quot;&quot;&quot;

    def invoke(self, data: BaseModel) -&gt; BaseModel:
        return data

</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM05: Supply Chain Vulnerabilities" data-index="52">
                    <div class="finding-header">
                        <span class="finding-title">Network fetch combined with code execution</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/mistralai/langchain_mistralai/chat_models.py:86</div>
                    <div class="description">This file downloads external content (lines [466, 616]) and executes code (lines [86]). This pattern enables remote code execution attacks if the fetched content is not properly validated.</div>
                    <div class="code-block"><code>
logger = logging.getLogger(__name__)

# Mistral enforces a specific pattern for tool call IDs
TOOL_CALL_ID_PATTERN = re.compile(r&quot;^[a-zA-Z0-9]{9}$&quot;)


# This SSL context is equivalent to the default `verify=True`.
# https://www.python-httpx.org/advanced/ssl/#configuring-client-instances
global_ssl_context = ssl.create_default_context(cafile=certifi.where())</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM06: Sensitive Information Disclosure" data-index="53">
                    <div class="finding-header">
                        <span class="finding-title">Hardcoded Generic API Key detected in assignment</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM06: Sensitive Information Disclosure</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/mistralai/langchain_mistralai/chat_models.py:120</div>
                    <div class="description">Hardcoded Generic API Key found in assignment on line 120. Hardcoded secrets in source code pose a critical security risk as they can be extracted by anyone with access to the codebase, version control history, or compiled binaries.</div>
                    <div class="code-block"><code>def _base62_encode(num: int) -&gt; str:
    &quot;&quot;&quot;Encode a number in base62 and ensures result is of a specified length.&quot;&quot;&quot;
    base62 = &quot;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ&quot;
    if num == 0:
        return base62[0]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Remove hardcoded secrets immediately:
1. Use environment variables: os.getenv(&#x27;API_KEY&#x27;)
2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault
3. Use configuration files (never commit to git): config.ini, .env
4. Rotate the exposed secret immediately
5. Scan git history for leaked secrets: git-secrets, truffleHog
6. Add secret scanning to CI/CD pipeline
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="54">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/embeddings/base.py:90</div>
                    <div class="description">Function &#x27;_parse_model_string&#x27; on line 90 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _parse_model_string(model_name: str) -&gt; tuple[str, str]:
    &quot;&quot;&quot;Parse a model string into provider and model name components.

    The model string should be in the format &#x27;provider:model-name&#x27;, where provider
    is one of the supported providers.

    Args:
        model_name: A model string in the format &#x27;provider:model-name&#x27;

    Returns:
        A tuple of (provider, model_name)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="55">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/embeddings/base.py:144</div>
                    <div class="description">Function &#x27;_infer_model_and_provider&#x27; on line 144 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _infer_model_and_provider(
    model: str,
    *,
    provider: str | None = None,
) -&gt; tuple[str, str]:
    if not model.strip():
        msg = &quot;Model name cannot be empty&quot;
        raise ValueError(msg)
    if provider is None and &quot;:&quot; in model:
        provider, model_name = _parse_model_string(model)
    else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="56">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/chat_models/base.py:701</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self._model(config).invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Any:
        return self._model(config).invoke(input, config=config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="57">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/chat_models/base.py:695</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 695 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        **kwargs: Any,
    ) -&gt; Any:
        return self._model(config).invoke(input, config=config, **kwargs)

    @override
    async def ainvoke(
        self,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="58">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;request&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1108</div>
                    <div class="description">User input &#x27;request&#x27; flows to LLM call via assignment in variable &#x27;messages&#x27;. Function &#x27;_execute_model_sync&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        model_, effective_response_format = _get_bound_model(request)
        messages = request.messages
        if request.system_message:
            messages = [request.system_message, *messages]

        output = model_.invoke(messages)
        if name:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="59">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:543</div>
                    <div class="description">Function &#x27;create_agent&#x27; on line 543 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_agent(
    model: str | BaseChatModel,
    tools: Sequence[BaseTool | Callable[..., Any] | dict[str, Any]] | None = None,
    *,
    system_prompt: str | SystemMessage | None = None,
    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),
    response_format: ResponseFormat[ResponseT] | type[ResponseT] | dict[str, Any] | None = None,
    state_schema: type[AgentState[ResponseT]] | None = None,
    context_schema: type[ContextT] | None = None,
    checkpointer: Checkpointer | None = None,
    store: BaseStore | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="60">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:980</div>
                    <div class="description">Function &#x27;_get_bound_model&#x27; on line 980 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_bound_model(
        request: ModelRequest,
    ) -&gt; tuple[Runnable[Any, Any], ResponseFormat[Any] | None]:
        &quot;&quot;&quot;Get the model with appropriate tool bindings.

        Performs auto-detection of strategy if needed based on model capabilities.

        Args:
            request: The model request containing model, tools, and response format.

        Returns:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="61">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1100</div>
                    <div class="description">Function &#x27;_execute_model_sync&#x27; on line 1100 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _execute_model_sync(request: ModelRequest) -&gt; ModelResponse:
        &quot;&quot;&quot;Execute model and return response.

        This is the core model execution logic wrapped by `wrap_model_call` handlers.
        Raises any exceptions that occur during model invocation.
        &quot;&quot;&quot;
        # Get the bound model (with auto-detection if needed)
        model_, effective_response_format = _get_bound_model(request)
        messages = request.messages
        if request.system_message:
            messages = [request.system_message, *messages]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="62">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;create_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:543</div>
                    <div class="description">Function &#x27;create_agent&#x27; on line 543 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>
    return result


def create_agent(
    model: str | BaseChatModel,
    tools: Sequence[BaseTool | Callable[..., Any] | dict[str, Any]] | None = None,
    *,
    system_prompt: str | SystemMessage | None = None,
    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="63">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_execute_model_sync&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1100</div>
                    <div class="description">Function &#x27;_execute_model_sync&#x27; on line 1100 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return request.model.bind(**request.model_settings), None

    def _execute_model_sync(request: ModelRequest) -&gt; ModelResponse:
        &quot;&quot;&quot;Execute model and return response.

        This is the core model execution logic wrapped by `wrap_model_call` handlers.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="64">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;_execute_model_sync&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1100</div>
                    <div class="description">Function &#x27;_execute_model_sync&#x27; on line 1100 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return request.model.bind(**request.model_settings), None

    def _execute_model_sync(request: ModelRequest) -&gt; ModelResponse:
        &quot;&quot;&quot;Execute model and return response.

        This is the core model execution logic wrapped by `wrap_model_call` handlers.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="65">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;request&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:134</div>
                    <div class="description">User input &#x27;request&#x27; flows to LLM call via f-string in variable &#x27;prompt&#x27;. Function &#x27;wrap_tool_call&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        # Extract tool information for emulation
        tool_args = request.tool_call[&quot;args&quot;]
        tool_description = request.tool.description if request.tool else &quot;No description available&quot;

        # Build prompt for emulator LLM
        prompt = (
            f&quot;You are emulating a tool call for testing purposes.\n\n&quot;
            f&quot;Tool: {tool_name}\n&quot;
            f&quot;Description: {tool_description}\n&quot;
            f&quot;Arguments: {tool_args}\n\n&quot;
            f&quot;Generate a realistic response that this tool would return &quot;
            f&quot;given these arguments.\n&quot;
            f&quot;Return ONLY the tool&#x27;s output, no explanation or preamble. &quot;
            f&quot;Introduce variation into your responses.&quot;
        )

        # Get emulated response from LLM
        response = self.model.invoke([HumanMessage(prompt)])
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="66">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:150</div>
                    <div class="description">LLM output from &#x27;self.model.invoke&#x27; is used in &#x27;call(&#x27; on line 150 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        # Get emulated response from LLM
        response = self.model.invoke([HumanMessage(prompt)])

        # Short-circuit: return emulated result without executing real tool</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="67">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;request&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:288</div>
                    <div class="description">User input &#x27;request&#x27; flows to LLM call via call in variable &#x27;selection_request&#x27;. Function &#x27;wrap_model_call&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        selection_request = self._prepare_selection_request(request)
        if selection_request is None:
            return handler(request)

        # Create dynamic response model with Literal enum of available tool names
        type_adapter = _create_tool_selection_response(selection_request.available_tools)
        schema = type_adapter.json_schema()
        structured_model = selection_request.model.with_structured_output(schema)

        response = structured_model.invoke(
            [</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="68">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:295</div>
                    <div class="description">LLM output from &#x27;selection_request.model.with_structured_output&#x27; is used in &#x27;SELECT&#x27; on line 295 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        schema = type_adapter.json_schema()
        structured_model = selection_request.model.with_structured_output(schema)

        response = structured_model.invoke(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="69">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:297</div>
                    <div class="description">LLM output from &#x27;structured_model.invoke&#x27; is used in &#x27;SELECT&#x27; on line 297 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        response = structured_model.invoke(
            [
                {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: selection_request.system_message},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="70">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;wrap_model_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:270</div>
                    <div class="description">Function &#x27;wrap_model_call&#x27; on line 270 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return request.override(tools=[*selected_tools, *provider_tools])

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="71">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/summarization.py:562</div>
                    <div class="description">Function &#x27;_create_summary&#x27; on line 562 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_summary(self, messages_to_summarize: list[AnyMessage]) -&gt; str:
        &quot;&quot;&quot;Generate summary for the given messages.&quot;&quot;&quot;
        if not messages_to_summarize:
            return &quot;No previous conversation history.&quot;

        trimmed_messages = self._trim_messages_for_summary(messages_to_summarize)
        if not trimmed_messages:
            return &quot;Previous conversation was too long to summarize.&quot;

        # Format messages to avoid token inflation from metadata when str() is called on
        # message objects</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="72">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;_create_summary&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/summarization.py:562</div>
                    <div class="description">Function &#x27;_create_summary&#x27; on line 562 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return idx

    def _create_summary(self, messages_to_summarize: list[AnyMessage]) -&gt; str:
        &quot;&quot;&quot;Generate summary for the given messages.&quot;&quot;&quot;
        if not messages_to_summarize:
            return &quot;No previous conversation history.&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="73">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:245</div>
                    <div class="description">LLM output from &#x27;request.model.get_num_tokens_from_messages&#x27; is used in &#x27;call(&#x27; on line 245 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="74">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:218</div>
                    <div class="description">Function &#x27;wrap_model_call&#x27; on line 218 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],
    ) -&gt; ModelCallResult:
        &quot;&quot;&quot;Apply context edits before invoking the model via handler.

        Args:
            request: Model request to execute (includes state and runtime).
            handler: Async callback that executes the model request and returns
                `ModelResponse`.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="75">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:244</div>
                    <div class="description">Function &#x27;count_tokens&#x27; on line 244 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )

        edited_messages = deepcopy(list(request.messages))
        for edit in self.edits:
            edit.apply(edited_messages, count_tokens=count_tokens)

        return handler(request.override(messages=edited_messages))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="76">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:281</div>
                    <div class="description">Function &#x27;count_tokens&#x27; on line 281 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )

        edited_messages = deepcopy(list(request.messages))
        for edit in self.edits:
            edit.apply(edited_messages, count_tokens=count_tokens)

        return await handler(request.override(messages=edited_messages))
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="77">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;wrap_model_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:218</div>
                    <div class="description">Function &#x27;wrap_model_call&#x27; on line 218 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        self.token_count_method = token_count_method

    def wrap_model_call(
        self,
        request: ModelRequest,
        handler: Callable[[ModelRequest], ModelResponse],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="78">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;count_tokens&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:244</div>
                    <div class="description">Function &#x27;count_tokens&#x27; on line 244 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            system_msg = [request.system_message] if request.system_message else []

            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="79">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;count_tokens&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:281</div>
                    <div class="description">Function &#x27;count_tokens&#x27; on line 281 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            system_msg = [request.system_message] if request.system_message else []

            def count_tokens(messages: Sequence[BaseMessage]) -&gt; int:
                return request.model.get_num_tokens_from_messages(
                    system_msg + list(messages), request.tools
                )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="80">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/contextual_compression.py:34</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.base_retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; list[Document]:
        docs = self.base_retriever.invoke(
            query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="81">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/contextual_compression.py:27</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 27 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
        **kwargs: Any,
    ) -&gt; list[Document]:
        docs = self.base_retriever.invoke(
            query,
            config={&quot;callbacks&quot;: run_manager.get_child()},
            **kwargs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="82">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/contextual_compression.py:27</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 27 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def _get_relevant_documents(
        self,
        query: str,
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="83">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/merger_retriever.py:69</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        retriever_docs = [
            retriever.invoke(
                query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="84">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/merger_retriever.py:53</div>
                    <div class="description">Function &#x27;merge_documents&#x27; on line 53 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def merge_documents(
        self,
        query: str,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Merge the results of the retrievers.

        Args:
            query: The query to search for.
            run_manager: The callback handler to use.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="85">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;merge_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/merger_retriever.py:53</div>
                    <div class="description">Function &#x27;merge_documents&#x27; on line 53 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return await self.amerge_documents(query, run_manager)

    def merge_documents(
        self,
        query: str,
        run_manager: CallbackManagerForRetrieverRun,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="86">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/re_phraser.py:76</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.llm_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        re_phrased_question = self.llm_chain.invoke(
            query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="87">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/re_phraser.py:61</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 61 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Get relevant documents given a user question.

        Args:
            query: user question
            run_manager: callback handler to use</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="88">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/re_phraser.py:61</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 61 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        )

    def _get_relevant_documents(
        self,
        query: str,
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="89">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/ensemble.py:224</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        retriever_docs = [
            retriever.invoke(
                query,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="90">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/ensemble.py:202</div>
                    <div class="description">Function &#x27;rank_fusion&#x27; on line 202 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def rank_fusion(
        self,
        query: str,
        run_manager: CallbackManagerForRetrieverRun,
        *,
        config: RunnableConfig | None = None,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Rank fusion.

        Retrieve the results of the retrievers and use rank_fusion_func to get
        the final result.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="91">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;rank_fusion&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/ensemble.py:202</div>
                    <div class="description">Function &#x27;rank_fusion&#x27; on line 202 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return await self.arank_fusion(query, run_manager)

    def rank_fusion(
        self,
        query: str,
        run_manager: CallbackManagerForRetrieverRun,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="92">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:179</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.generate_queries&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        queries = self.generate_queries(query, run_manager)
        if self.include_original:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="93">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;question&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:199</div>
                    <div class="description">User input parameter &#x27;question&#x27; is directly passed to LLM API call &#x27;self.llm_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;
        response = self.llm_chain.invoke(
            {&quot;question&quot;: question},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="94">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:164</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 164 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Get relevant documents given a user query.

        Args:
            query: user query
            run_manager: the callback handler to use.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="95">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:185</div>
                    <div class="description">Function &#x27;generate_queries&#x27; on line 185 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate_queries(
        self,
        question: str,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[str]:
        &quot;&quot;&quot;Generate queries based upon user input.

        Args:
            question: user query
            run_manager: run manager for callbacks
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="96">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:164</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 164 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return [doc for docs in document_lists for doc in docs]

    def _get_relevant_documents(
        self,
        query: str,
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="97">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;generate_queries&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:185</div>
                    <div class="description">Function &#x27;generate_queries&#x27; on line 185 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return self.unique_union(documents)

    def generate_queries(
        self,
        question: str,
        run_manager: CallbackManagerForRetrieverRun,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="98">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/embeddings/base.py:28</div>
                    <div class="description">Function &#x27;_parse_model_string&#x27; on line 28 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _parse_model_string(model_name: str) -&gt; tuple[str, str]:
    &quot;&quot;&quot;Parse a model string into provider and model name components.

    The model string should be in the format &#x27;provider:model-name&#x27;, where provider
    is one of the supported providers.

    Args:
        model_name: A model string in the format &#x27;provider:model-name&#x27;

    Returns:
        A tuple of (provider, model_name)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="99">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/embeddings/base.py:83</div>
                    <div class="description">Function &#x27;_infer_model_and_provider&#x27; on line 83 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _infer_model_and_provider(
    model: str,
    *,
    provider: str | None = None,
) -&gt; tuple[str, str]:
    if not model.strip():
        msg = &quot;Model name cannot be empty&quot;
        raise ValueError(msg)
    if provider is None and &quot;:&quot; in model:
        provider, model_name = _parse_model_string(model)
    else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="100">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/chat_memory.py:74</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 74 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer.&quot;&quot;&quot;
        input_str, output_str = self._get_input_output(inputs, outputs)
        self.chat_memory.add_messages(
            [
                HumanMessage(content=input_str),
                AIMessage(content=output_str),
            ],
        )

    async def asave_context(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="101">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/chat_memory.py:98</div>
                    <div class="description">Function &#x27;clear&#x27; on line 98 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def clear(self) -&gt; None:
        &quot;&quot;&quot;Clear memory contents.&quot;&quot;&quot;
        self.chat_memory.clear()

    async def aclear(self) -&gt; None:
        &quot;&quot;&quot;Clear memory contents.&quot;&quot;&quot;
        await self.chat_memory.aclear()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="102">
                    <div class="finding-header">
                        <span class="finding-title">High-risk network operation without confirmation in &#x27;save_context&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/chat_memory.py:74</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 74 performs high-risk network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return inputs[prompt_input_key], outputs[output_key]

    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer.&quot;&quot;&quot;
        input_str, output_str = self._get_input_output(inputs, outputs)
        self.chat_memory.add_messages(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="103">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/summary_buffer.py:112</div>
                    <div class="description">Function &#x27;prune&#x27; on line 112 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def prune(self) -&gt; None:
        &quot;&quot;&quot;Prune buffer if it exceeds max token limit.&quot;&quot;&quot;
        buffer = self.chat_memory.messages
        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)
        if curr_buffer_length &gt; self.max_token_limit:
            pruned_memory = []
            while curr_buffer_length &gt; self.max_token_limit:
                pruned_memory.append(buffer.pop(0))
                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)
            self.moving_summary_buffer = self.predict_new_summary(
                pruned_memory,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="104">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;prune&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/summary_buffer.py:112</div>
                    <div class="description">Function &#x27;prune&#x27; on line 112 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        await self.aprune()

    def prune(self) -&gt; None:
        &quot;&quot;&quot;Prune buffer if it exceeds max token limit.&quot;&quot;&quot;
        buffer = self.chat_memory.messages
        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="105">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;prune&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/summary_buffer.py:112</div>
                    <div class="description">Function &#x27;prune&#x27; on line 112 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        await self.aprune()

    def prune(self) -&gt; None:
        &quot;&quot;&quot;Prune buffer if it exceeds max token limit.&quot;&quot;&quot;
        buffer = self.chat_memory.messages
        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="106">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py:145</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 145 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer. Pruned.&quot;&quot;&quot;
        BaseChatMemory.save_context(self, inputs, outputs)
        self._timestamps.append(datetime.now().astimezone())
        # Prune buffer if it exceeds max token limit
        buffer = self.chat_memory.messages
        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)
        if curr_buffer_length &gt; self.max_token_limit:
            while curr_buffer_length &gt; self.max_token_limit:
                self._pop_and_store_interaction(buffer)
                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="107">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;save_context&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py:145</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 145 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return {self.memory_key: messages}

    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer. Pruned.&quot;&quot;&quot;
        BaseChatMemory.save_context(self, inputs, outputs)
        self._timestamps.append(datetime.now().astimezone())</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="108">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore.py:73</div>
                    <div class="description">User input &#x27;inputs&#x27; flows to LLM call via assignment in variable &#x27;query&#x27;. Function &#x27;load_memory_variables&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>        input_key = self._get_prompt_input_key(inputs)
        query = inputs[input_key]
        docs = self.retriever.invoke(query)
        return self._documents_to_memory_variables(docs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="109">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore.py:67</div>
                    <div class="description">Function &#x27;load_memory_variables&#x27; on line 67 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def load_memory_variables(
        self,
        inputs: dict[str, Any],
    ) -&gt; dict[str, list[Document] | str]:
        &quot;&quot;&quot;Return history buffer.&quot;&quot;&quot;
        input_key = self._get_prompt_input_key(inputs)
        query = inputs[input_key]
        docs = self.retriever.invoke(query)
        return self._documents_to_memory_variables(docs)

    async def aload_memory_variables(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="110">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/entity.py:607</div>
                    <div class="description">Function &#x27;clear&#x27; on line 607 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def clear(self) -&gt; None:
        &quot;&quot;&quot;Clear memory contents.&quot;&quot;&quot;
        self.chat_memory.clear()
        self.entity_cache.clear()
        self.entity_store.clear()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="111">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/token_buffer.py:61</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 61 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer. Pruned.&quot;&quot;&quot;
        super().save_context(inputs, outputs)
        # Prune buffer if it exceeds max token limit
        buffer = self.chat_memory.messages
        curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)
        if curr_buffer_length &gt; self.max_token_limit:
            pruned_memory = []
            while curr_buffer_length &gt; self.max_token_limit:
                pruned_memory.append(buffer.pop(0))
                curr_buffer_length = self.llm.get_num_tokens_from_messages(buffer)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="112">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;save_context&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/token_buffer.py:61</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 61 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return {self.memory_key: self.buffer}

    def save_context(self, inputs: dict[str, Any], outputs: dict[str, str]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer. Pruned.&quot;&quot;&quot;
        super().save_context(inputs, outputs)
        # Prune buffer if it exceeds max token limit</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="113">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chat_models/base.py:773</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self._model(config).invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Any:
        return self._model(config).invoke(input, config=config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="114">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chat_models/base.py:767</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 767 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        **kwargs: Any,
    ) -&gt; Any:
        return self._model(config).invoke(input, config=config, **kwargs)

    @override
    async def ainvoke(
        self,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="115">
                    <div class="finding-header">
                        <span class="finding-title">Dynamic tool/plugin loading</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/__init__.py:56</div>
                    <div class="description">Dynamic tool calling without validation on line 56.</div>
                    <div class="code-block"><code>    create_structured_chat_agent,
)
from langchain_classic.agents.tool_calling_agent.base import create_tool_calling_agent
from langchain_classic.agents.xml.base import XMLAgent, create_xml_agent

if TYPE_CHECKING:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="116">
                    <div class="finding-header">
                        <span class="finding-title">Dynamic tool/plugin loading</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/__init__.py:154</div>
                    <div class="description">Dynamic tool calling without validation on line 154.</div>
                    <div class="code-block"><code>    &quot;create_sql_agent&quot;,
    &quot;create_structured_chat_agent&quot;,
    &quot;create_tool_calling_agent&quot;,
    &quot;create_vectorstore_agent&quot;,
    &quot;create_vectorstore_router_agent&quot;,
    &quot;create_xml_agent&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="117">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:419</div>
                    <div class="description">Function &#x27;plan&#x27; on line 419 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; AgentAction | AgentFinish:
        &quot;&quot;&quot;Based on past history and current inputs, decide what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with the observations.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="118">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:531</div>
                    <div class="description">Function &#x27;plan&#x27; on line 531 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; list[AgentAction] | AgentFinish:
        &quot;&quot;&quot;Based on past history and current inputs, decide what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with the observations.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="119">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:419</div>
                    <div class="description">Function &#x27;plan&#x27; on line 419 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return self.input_keys_arg

    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="120">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:531</div>
                    <div class="description">Function &#x27;plan&#x27; on line 531 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return self.input_keys_arg

    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="121">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:419</div>
                    <div class="description">Function &#x27;plan&#x27; on line 419 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.input_keys_arg

    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="122">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:531</div>
                    <div class="description">Function &#x27;plan&#x27; on line 531 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return self.input_keys_arg

    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="123">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompt_value&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:251</div>
                    <div class="description">User input parameter &#x27;prompt_value&#x27; is directly passed to LLM API call &#x27;self.retry_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                else:
                    completion = self.retry_chain.invoke(
                        {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="124">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:117</div>
                    <div class="description">LLM output from &#x27;self.retry_chain.invoke&#x27; is used in &#x27;run(&#x27; on line 117 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):
                    completion = self.retry_chain.run(
                        prompt=prompt_value.to_string(),
                        completion=completion,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="125">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:245</div>
                    <div class="description">LLM output from &#x27;self.retry_chain.invoke&#x27; is used in &#x27;run(&#x27; on line 245 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):
                    completion = self.retry_chain.run(
                        prompt=prompt_value.to_string(),
                        completion=completion,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="126">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:97</div>
                    <div class="description">Function &#x27;parse_with_prompt&#x27; on line 97 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -&gt; T:
        &quot;&quot;&quot;Parse the output of an LLM call using a wrapped parser.

        Args:
            completion: The chain completion to parse.
            prompt_value: The prompt to use to parse the completion.

        Returns:
            The parsed completion.
        &quot;&quot;&quot;
        retries = 0</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="127">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:234</div>
                    <div class="description">Function &#x27;parse_with_prompt&#x27; on line 234 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -&gt; T:
        retries = 0

        while retries &lt;= self.max_retries:
            try:
                return self.parser.parse(completion)
            except OutputParserException as e:
                if retries == self.max_retries:
                    raise
                retries += 1
                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="128">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/fix.py:81</div>
                    <div class="description">LLM output from &#x27;self.retry_chain.invoke&#x27; is used in &#x27;run(&#x27; on line 81 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):
                    completion = self.retry_chain.run(
                        instructions=self.parser.get_format_instructions(),
                        completion=completion,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="129">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/fix.py:70</div>
                    <div class="description">Function &#x27;parse&#x27; on line 70 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def parse(self, completion: str) -&gt; T:
        retries = 0

        while retries &lt;= self.max_retries:
            try:
                return self.parser.parse(completion)
            except OutputParserException as e:
                if retries == self.max_retries:
                    raise
                retries += 1
                if self.legacy and hasattr(self.retry_chain, &quot;run&quot;):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="130">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;parse&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/fix.py:70</div>
                    <div class="description">Function &#x27;parse&#x27; on line 70 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def parse(self, completion: str) -&gt; T:
        retries = 0

        while retries &lt;= self.max_retries:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="131">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/loading.py:115</div>
                    <div class="description">Function &#x27;load_evaluator&#x27; on line 115 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def load_evaluator(
    evaluator: EvaluatorType,
    *,
    llm: BaseLanguageModel | None = None,
    **kwargs: Any,
) -&gt; Chain | StringEvaluator:
    &quot;&quot;&quot;Load the requested evaluation chain specified by a string.

    Parameters
    ----------
    evaluator : EvaluatorType</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="132">
                    <div class="finding-header">
                        <span class="finding-title">Unpinned model version in API call</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/loading.py:168</div>
                    <div class="description">Model &#x27;&#x27;gpt-4&#x27;&#x27; is used without version pinning on line 168. Unpinned models can change unexpectedly, introducing breaking changes, security vulnerabilities, or behavioral shifts. This file contains dynamic code execution patterns, increasing risk.</div>
                    <div class="code-block"><code>                    raise ImportError(msg) from e

            llm = llm or ChatOpenAI(model=&quot;gpt-4&quot;, seed=42, temperature=0)
        except Exception as e:
            msg = (
                f&quot;Evaluation with the {evaluator_cls} requires a &quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Supply Chain Security Best Practices:
1. Pin model versions explicitly (model=&#x27;gpt-4-0613&#x27;)
2. Use model registries with version control
3. Document model versions in requirements.txt or similar
4. Implement model versioning in CI/CD pipelines
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="133">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;load_evaluator&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/loading.py:115</div>
                    <div class="description">Function &#x27;load_evaluator&#x27; on line 115 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def load_evaluator(
    evaluator: EvaluatorType,
    *,
    llm: BaseLanguageModel | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="134">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/example_generator.py:9</div>
                    <div class="description">Function &#x27;generate_example&#x27; on line 9 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def generate_example(
    examples: list[dict],
    llm: BaseLanguageModel,
    prompt_template: PromptTemplate,
) -&gt; str:
    &quot;&quot;&quot;Return another example given a list of examples for a prompt.&quot;&quot;&quot;
    prompt = FewShotPromptTemplate(
        examples=examples,
        suffix=TEST_GEN_TEMPLATE_SUFFIX,
        input_variables=[],
        example_prompt=prompt_template,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="135">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:117</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; dict[str, str]:
        response = self.generate([inputs], run_manager=run_manager)
        return self.create_outputs(response)[0]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="136">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_list&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:241</div>
                    <div class="description">User input parameter &#x27;input_list&#x27; is directly passed to LLM API call &#x27;self.generate&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        try:
            response = self.generate(input_list, run_manager=run_manager)
        except BaseException as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="137">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:112</div>
                    <div class="description">Function &#x27;_call&#x27; on line 112 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        response = self.generate([inputs], run_manager=run_manager)
        return self.create_outputs(response)[0]

    def generate(
        self,
        input_list: list[dict[str, Any]],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="138">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:120</div>
                    <div class="description">Function &#x27;generate&#x27; on line 120 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate(
        self,
        input_list: list[dict[str, Any]],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; LLMResult:
        &quot;&quot;&quot;Generate LLM result from inputs.&quot;&quot;&quot;
        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)
        callbacks = run_manager.get_child() if run_manager else None
        if isinstance(self.llm, BaseLanguageModel):
            return self.llm.generate_prompt(
                prompts,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="139">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:224</div>
                    <div class="description">Function &#x27;apply&#x27; on line 224 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def apply(
        self,
        input_list: list[dict[str, Any]],
        callbacks: Callbacks = None,
    ) -&gt; list[dict[str, str]]:
        &quot;&quot;&quot;Utilize the LLM generate method for speed gains.&quot;&quot;&quot;
        callback_manager = CallbackManager.configure(
            callbacks,
            self.callbacks,
            self.verbose,
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="140">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:112</div>
                    <div class="description">Function &#x27;_call&#x27; on line 112 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return [self.output_key, &quot;full_generation&quot;]

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="141">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;apply&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:224</div>
                    <div class="description">Function &#x27;apply&#x27; on line 224 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return prompts, stop

    def apply(
        self,
        input_list: list[dict[str, Any]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="142">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;apply&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:224</div>
                    <div class="description">Function &#x27;apply&#x27; on line 224 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return prompts, stop

    def apply(
        self,
        input_list: list[dict[str, Any]],
        callbacks: Callbacks = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="143">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:413</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
        return self.invoke(
            inputs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="144">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:413</div>
                    <div class="description">LLM output from &#x27;self.invoke&#x27; is used in &#x27;call(&#x27; on line 413 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>
        return self.invoke(
            inputs,
            cast(&quot;RunnableConfig&quot;, {k: v for k, v in config.items() if v is not None}),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="145">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:369</div>
                    <div class="description">Function &#x27;__call__&#x27; on line 369 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __call__(
        self,
        inputs: dict[str, Any] | Any,
        return_only_outputs: bool = False,  # noqa: FBT001,FBT002
        callbacks: Callbacks = None,
        *,
        tags: list[str] | None = None,
        metadata: dict[str, Any] | None = None,
        run_name: str | None = None,
        include_run_info: bool = False,
    ) -&gt; dict[str, Any]:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="146">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;__call__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:369</div>
                    <div class="description">Function &#x27;__call__&#x27; on line 369 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @deprecated(&quot;0.1.0&quot;, alternative=&quot;invoke&quot;, removal=&quot;1.0&quot;)
    def __call__(
        self,
        inputs: dict[str, Any] | Any,
        return_only_outputs: bool = False,  # noqa: FBT001,FBT002</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="147">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;question&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:67</div>
                    <div class="description">User input parameter &#x27;question&#x27; is directly passed to LLM API call &#x27;chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        )
        return chain.invoke({chain.input_key: question})[chain.output_key]
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="148">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;question&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:137</div>
                    <div class="description">User input parameter &#x27;question&#x27; is directly passed to LLM API call &#x27;chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        )
        return chain.invoke({chain.question_key: question})
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="149">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:34</div>
                    <div class="description">Function &#x27;query&#x27; on line 34 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query(
        self,
        question: str,
        llm: BaseLanguageModel | None = None,
        retriever_kwargs: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -&gt; str:
        &quot;&quot;&quot;Query the `VectorStore` using the provided LLM.

        Args:
            question: The question or prompt to query.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="150">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:104</div>
                    <div class="description">Function &#x27;query_with_sources&#x27; on line 104 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def query_with_sources(
        self,
        question: str,
        llm: BaseLanguageModel | None = None,
        retriever_kwargs: dict[str, Any] | None = None,
        **kwargs: Any,
    ) -&gt; dict:
        &quot;&quot;&quot;Query the `VectorStore` and retrieve the answer along with sources.

        Args:
            question: The question or prompt to query.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="151">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;query&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:34</div>
                    <div class="description">Function &#x27;query&#x27; on line 34 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>    )

    def query(
        self,
        question: str,
        llm: BaseLanguageModel | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="152">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;query_with_sources&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:104</div>
                    <div class="description">Function &#x27;query_with_sources&#x27; on line 104 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>        return (await chain.ainvoke({chain.input_key: question}))[chain.output_key]

    def query_with_sources(
        self,
        question: str,
        llm: BaseLanguageModel | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="153">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;text&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:81</div>
                    <div class="description">User input parameter &#x27;text&#x27; is directly passed to LLM API call &#x27;self.llm_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        var_name = self.input_keys[0]
        result = self.llm_chain.invoke({var_name: text})
        if isinstance(self.llm_chain, LLMChain):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="154">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:96</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.llm_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        return self.llm_chain.invoke(
            inputs,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="155">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous command_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:81</div>
                    <div class="description">LLM output from &#x27;self.llm_chain.invoke&#x27; is used in &#x27;call(&#x27; on line 81 without sanitization. This creates a command_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        var_name = self.input_keys[0]
        result = self.llm_chain.invoke({var_name: text})
        if isinstance(self.llm_chain, LLMChain):
            documents = [result[self.output_keys[0]]]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="156">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:89</div>
                    <div class="description">Function &#x27;_call&#x27; on line 89 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        &quot;&quot;&quot;Call the internal llm chain.&quot;&quot;&quot;
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        return self.llm_chain.invoke(
            inputs,
            config={&quot;callbacks&quot;: _run_manager.get_child()},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="157">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:89</div>
                    <div class="description">Function &#x27;_call&#x27; on line 89 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return self.combine_embeddings(embeddings)

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="158">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:116</div>
                    <div class="description">Function &#x27;_call&#x27; on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        input_text = f&quot;{inputs[self.input_key]}\nESQuery:&quot;
        _run_manager.on_text(input_text, verbose=self.verbose)
        indices = self._list_indices()
        indices_info = self._get_indices_infos(indices)
        query_inputs: dict = {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM05: Supply Chain Vulnerabilities" data-index="159">
                    <div class="finding-header">
                        <span class="finding-title">Network fetch combined with code execution</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/openai_functions/openapi.py:98</div>
                    <div class="description">This file downloads external content (lines [196]) and executes code (lines [98, 335, 344]). This pattern enables remote code execution attacks if the fetched content is not properly validated.</div>
                    <div class="code-block"><code>    Args:
        spec: OpenAPI spec to convert.

    Returns:
        Tuple of the OpenAI functions JSON schema and a default function for executing
            a request based on the OpenAI function schema.
    &quot;&quot;&quot;
    try:
        from langchain_community.tools import APIOperation
    except ImportError as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="160">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py:77</div>
                    <div class="description">Function &#x27;create_citation_fuzzy_match_runnable&#x27; on line 77 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_citation_fuzzy_match_runnable(llm: BaseChatModel) -&gt; Runnable:
    &quot;&quot;&quot;Create a citation fuzzy match Runnable.

    Example usage:

        ```python
        from langchain_classic.chains import create_citation_fuzzy_match_runnable
        from langchain_openai import ChatOpenAI

        model = ChatOpenAI(model=&quot;gpt-4o-mini&quot;)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="161">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;create_citation_fuzzy_match_runnable&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py:77</div>
                    <div class="description">Function &#x27;create_citation_fuzzy_match_runnable&#x27; on line 77 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_citation_fuzzy_match_runnable(llm: BaseChatModel) -&gt; Runnable:
    &quot;&quot;&quot;Create a citation fuzzy match Runnable.

    Example usage:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="162">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/sql_database/query.py:33</div>
                    <div class="description">Function &#x27;create_sql_query_chain&#x27; on line 33 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_sql_query_chain(
    llm: BaseLanguageModel,
    db: SQLDatabase,
    prompt: BasePromptTemplate | None = None,
    k: int = 5,
    *,
    get_col_comments: bool | None = None,
) -&gt; Runnable[SQLInput | SQLInputWithTables | dict[str, Any], str]:
    r&quot;&quot;&quot;Create a chain that generates SQL queries.

    *Security Note*: This chain generates SQL queries for the given database.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="163">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_sql_query_chain&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/sql_database/query.py:33</div>
                    <div class="description">Function &#x27;create_sql_query_chain&#x27; on line 33 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_sql_query_chain(
    llm: BaseLanguageModel,
    db: SQLDatabase,
    prompt: BasePromptTemplate | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="164">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:66</div>
                    <div class="description">Function &#x27;create_openai_fn_runnable&#x27; on line 66 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_openai_fn_runnable(
    functions: Sequence[dict[str, Any] | type[BaseModel] | Callable],
    llm: Runnable,
    prompt: BasePromptTemplate | None = None,
    *,
    enforce_single_function_usage: bool = True,
    output_parser: BaseOutputParser | BaseGenerationOutputParser | None = None,
    **llm_kwargs: Any,
) -&gt; Runnable:
    &quot;&quot;&quot;Create a runnable sequence that uses OpenAI functions.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="165">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:450</div>
                    <div class="description">Function &#x27;_create_openai_tools_runnable&#x27; on line 450 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _create_openai_tools_runnable(
    tool: dict[str, Any] | type[BaseModel] | Callable,
    llm: Runnable,
    *,
    prompt: BasePromptTemplate | None,
    output_parser: BaseOutputParser | BaseGenerationOutputParser | None,
    enforce_tool_usage: bool,
    first_tool_only: bool,
) -&gt; Runnable:
    oai_tool = convert_to_openai_tool(tool)
    llm_kwargs: dict[str, Any] = {&quot;tools&quot;: [oai_tool]}</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="166">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:524</div>
                    <div class="description">Function &#x27;_create_openai_json_runnable&#x27; on line 524 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _create_openai_json_runnable(
    output_schema: dict[str, Any] | type[BaseModel],
    llm: Runnable,
    prompt: BasePromptTemplate | None = None,
    *,
    output_parser: BaseOutputParser | BaseGenerationOutputParser | None = None,
) -&gt; Runnable:
    if isinstance(output_schema, type) and is_basemodel_subclass(output_schema):
        output_parser = output_parser or PydanticOutputParser(
            pydantic_object=output_schema,
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="167">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;_create_openai_tools_runnable&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:450</div>
                    <div class="description">Function &#x27;_create_openai_tools_runnable&#x27; on line 450 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _create_openai_tools_runnable(
    tool: dict[str, Any] | type[BaseModel] | Callable,
    llm: Runnable,
    *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="168">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:268</div>
                    <div class="description">Function &#x27;_get_docs&#x27; on line 268 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_docs(
        self,
        question: str,
        *,
        run_manager: CallbackManagerForChainRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Get docs.&quot;&quot;&quot;
        return self.retriever.invoke(
            question,
            config={&quot;callbacks&quot;: run_manager.get_child()},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="169">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;_get_docs&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:268</div>
                    <div class="description">Function &#x27;_get_docs&#x27; on line 268 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>    retriever: BaseRetriever = Field(exclude=True)

    def _get_docs(
        self,
        question: str,
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="170">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_with_sources/retrieval.py:52</div>
                    <div class="description">User input &#x27;inputs&#x27; flows to LLM call via assignment in variable &#x27;question&#x27;. Function &#x27;_get_docs&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>    ) -&gt; list[Document]:
        question = inputs[self.question_key]
        docs = self.retriever.invoke(
            question,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="171">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_with_sources/retrieval.py:46</div>
                    <div class="description">Function &#x27;_get_docs&#x27; on line 46 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_docs(
        self,
        inputs: dict[str, Any],
        *,
        run_manager: CallbackManagerForChainRun,
    ) -&gt; list[Document]:
        question = inputs[self.question_key]
        docs = self.retriever.invoke(
            question,
            config={&quot;callbacks&quot;: run_manager.get_child()},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="172">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_get_docs&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_with_sources/retrieval.py:46</div>
                    <div class="description">Function &#x27;_get_docs&#x27; on line 46 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return docs[:num_docs]

    def _get_docs(
        self,
        inputs: dict[str, Any],
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="173">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_generation/base.py:121</div>
                    <div class="description">User input &#x27;inputs&#x27; flows to LLM call via call in variable &#x27;docs&#x27;. Function &#x27;_call&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>    ) -&gt; dict[str, list]:
        docs = self.text_splitter.create_documents([inputs[self.input_key]])
        results = self.llm_chain.generate(
            [{&quot;text&quot;: d.page_content} for d in docs],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="174">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_generation/base.py:116</div>
                    <div class="description">Function &#x27;_call&#x27; on line 116 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, list]:
        docs = self.text_splitter.create_documents([inputs[self.input_key]])
        results = self.llm_chain.generate(
            [{&quot;text&quot;: d.page_content} for d in docs],
            run_manager=run_manager,
        )
        qa = [json.loads(res[0].text) for res in results.generations]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="175">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_generation/base.py:116</div>
                    <div class="description">Function &#x27;_call&#x27; on line 116 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return [self.output_key]

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="176">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/natbot/base.py:113</div>
                    <div class="description">Function &#x27;_call&#x27; on line 113 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, str]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()
        url = inputs[self.input_url_key]
        browser_content = inputs[self.input_browser_content_key]
        llm_cmd = self.llm_chain.invoke(
            {
                &quot;objective&quot;: self.objective,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="177">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/natbot/base.py:113</div>
                    <div class="description">Function &#x27;_call&#x27; on line 113 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return [self.output_key]

    def _call(
        self,
        inputs: dict[str, str],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="178">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py:408</div>
                    <div class="description">Function &#x27;_get_docs&#x27; on line 408 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_docs(
        self,
        question: str,
        inputs: dict[str, Any],
        *,
        run_manager: CallbackManagerForChainRun,
    ) -&gt; list[Document]:
        &quot;&quot;&quot;Get docs.&quot;&quot;&quot;
        docs = self.retriever.invoke(
            question,
            config={&quot;callbacks&quot;: run_manager.get_child()},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="179">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_get_docs&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py:408</div>
                    <div class="description">Function &#x27;_get_docs&#x27; on line 408 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def _get_docs(
        self,
        question: str,
        inputs: dict[str, Any],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="180">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;user_input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:147</div>
                    <div class="description">User input parameter &#x27;user_input&#x27; is directly passed to LLM API call &#x27;self.response_chain.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        context = &quot;\n\n&quot;.join(d.page_content for d in docs)
        result = self.response_chain.invoke(
            {</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="181">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:135</div>
                    <div class="description">Function &#x27;_do_generation&#x27; on line 135 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _do_generation(
        self,
        questions: list[str],
        user_input: str,
        response: str,
        _run_manager: CallbackManagerForChainRun,
    ) -&gt; tuple[str, bool]:
        callbacks = _run_manager.get_child()
        docs = []
        for question in questions:
            docs.extend(self.retriever.invoke(question))</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="182">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:198</div>
                    <div class="description">Function &#x27;_call&#x27; on line 198 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()

        user_input = inputs[self.input_keys[0]]

        response = &quot;&quot;
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="183">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:250</div>
                    <div class="description">Function &#x27;from_llm&#x27; on line 250 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def from_llm(
        cls,
        llm: BaseLanguageModel | None,
        max_generation_len: int = 32,
        **kwargs: Any,
    ) -&gt; FlareChain:
        &quot;&quot;&quot;Creates a FlareChain from a language model.

        Args:
            llm: Language model to use.
            max_generation_len: Maximum length of the generated response.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM05: Supply Chain Vulnerabilities" data-index="184">
                    <div class="finding-header">
                        <span class="finding-title">Code execution on external content</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:237</div>
                    <div class="description">eval() on non-literal content on line 237. </div>
                    <div class="code-block"><code>                continue

            marginal, finished = self._do_retrieval(
                low_confidence_spans,
                _run_manager,
                user_input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Code Execution:
1. NEVER use eval/exec on untrusted input
2. Use safe alternatives (json.loads, ast.literal_eval)
3. Validate and sanitize all external content
4. Use sandboxed execution environments
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM08: Excessive Agency" data-index="185">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM-generated code in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:198</div>
                    <div class="description">Function &#x27;_call&#x27; on line 198 directly executes code generated or influenced by an LLM using exec()/eval() or subprocess. This creates a critical security risk where malicious or buggy LLM outputs can execute arbitrary code, potentially compromising the entire system.</div>
                    <div class="code-block"><code>            end=&quot;\n&quot;,
        )
        return self._do_generation(questions, user_input, response, _run_manager)

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,
    ) -&gt; dict[str, Any]:
        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution

High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="186">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_do_generation&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:135</div>
                    <div class="description">Function &#x27;_do_generation&#x27; on line 135 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return [&quot;response&quot;]

    def _do_generation(
        self,
        questions: list[str],
        user_input: str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM09: Overreliance" data-index="187">
                    <div class="finding-header">
                        <span class="finding-title">Direct execution of LLM output in &#x27;_call&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:198</div>
                    <div class="description">Function &#x27;_call&#x27; on line 198 directly executes LLM-generated code using eval(. This is extremely dangerous and allows arbitrary code execution.</div>
                    <div class="code-block"><code>        return self._do_generation(questions, user_input, response, _run_manager)

    def _call(
        self,
        inputs: dict[str, Any],
        run_manager: CallbackManagerForChainRun | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="188">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;from_llm&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:250</div>
                    <div class="description">Function &#x27;from_llm&#x27; on line 250 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>
    @classmethod
    def from_llm(
        cls,
        llm: BaseLanguageModel | None,
        max_generation_len: int = 32,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="189">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/scoring/eval_chain.py:240</div>
                    <div class="description">Function &#x27;from_llm&#x27; on line 240 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def from_llm(
        cls,
        llm: BaseLanguageModel,
        *,
        prompt: PromptTemplate | None = None,
        criteria: CRITERIA_TYPE | str | None = None,
        normalize_by: float | None = None,
        **kwargs: Any,
    ) -&gt; ScoreStringEvalChain:
        &quot;&quot;&quot;Initialize the ScoreStringEvalChain from an LLM.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="190">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/comparison/eval_chain.py:240</div>
                    <div class="description">Function &#x27;from_llm&#x27; on line 240 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def from_llm(
        cls,
        llm: BaseLanguageModel,
        *,
        prompt: PromptTemplate | None = None,
        criteria: CRITERIA_TYPE | str | None = None,
        **kwargs: Any,
    ) -&gt; PairwiseStringEvalChain:
        &quot;&quot;&quot;Initialize the PairwiseStringEvalChain from an LLM.

        Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="191">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py:97</div>
                    <div class="description">Function &#x27;create_self_ask_with_search_agent&#x27; on line 97 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_self_ask_with_search_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,
) -&gt; Runnable:
    &quot;&quot;&quot;Create an agent that uses self-ask with search prompting.

    Args:
        llm: LLM to use as the agent.
        tools: List of tools. Should just be of length 1, with that tool having
            name `Intermediate Answer`</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="192">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;create_self_ask_with_search_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py:97</div>
                    <div class="description">Function &#x27;create_self_ask_with_search_agent&#x27; on line 97 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_self_ask_with_search_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="193">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_tools/base.py:17</div>
                    <div class="description">Function &#x27;create_openai_tools_agent&#x27; on line 17 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_openai_tools_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,
    strict: bool | None = None,  # noqa: FBT001
) -&gt; Runnable:
    &quot;&quot;&quot;Create an agent that uses OpenAI tools.

    Args:
        llm: LLM to use as the agent.
        tools: Tools this agent has access to.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="194">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;create_openai_tools_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_tools/base.py:100</div>
                    <div class="description">Function &#x27;create_openai_tools_agent&#x27; on line 17 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>        raise ValueError(msg)

    llm_with_tools = llm.bind(
        tools=[convert_to_openai_tool(tool, strict=strict) for tool in tools],
    )
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="195">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_openai_tools_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_tools/base.py:17</div>
                    <div class="description">Function &#x27;create_openai_tools_agent&#x27; on line 17 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_openai_tools_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="196">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py:75</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 75 has 3 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def save_context(self, inputs: dict[str, Any], outputs: dict[str, Any]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer. Pruned.

        Args:
            inputs: Inputs to the agent.
            outputs: Outputs from the agent.
        &quot;&quot;&quot;
        input_str, output_str = self._get_input_output(inputs, outputs)
        self.chat_memory.add_messages(input_str)  # type: ignore[arg-type]
        format_to_messages = (
            format_to_tool_messages</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="197">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;save_context&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py:75</div>
                    <div class="description">Function &#x27;save_context&#x27; on line 75 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return {self.memory_key: final_buffer}

    def save_context(self, inputs: dict[str, Any], outputs: dict[str, Any]) -&gt; None:
        &quot;&quot;&quot;Save context from this conversation to buffer. Pruned.

        Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="198">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:125</div>
                    <div class="description">LLM output from &#x27;self.llm.invoke&#x27; is used in &#x27;SELECT&#x27; on line 125 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        if with_functions:
            predicted_message = self.llm.invoke(
                messages,
                functions=self.functions,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="199">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:287</div>
                    <div class="description">Function &#x27;create_openai_functions_agent&#x27; on line 287 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_openai_functions_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,
) -&gt; Runnable:
    &quot;&quot;&quot;Create an agent that uses OpenAI function calling.

    Args:
        llm: LLM to use as the agent. Should work with OpenAI function calling,
            so either be an OpenAI model that supports that or a wrapper of
            a different model that adds in equivalent support.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="200">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:96</div>
                    <div class="description">Function &#x27;plan&#x27; on line 96 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,
        with_functions: bool = True,  # noqa: FBT001,FBT002
        **kwargs: Any,
    ) -&gt; AgentAction | AgentFinish:
        &quot;&quot;&quot;Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="201">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;create_openai_functions_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:372</div>
                    <div class="description">Function &#x27;create_openai_functions_agent&#x27; on line 287 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>        )
        raise ValueError(msg)
    llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])
    return (
        RunnablePassthrough.assign(
            agent_scratchpad=lambda x: format_to_openai_function_messages(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="202">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:125</div>
                    <div class="description">Function &#x27;plan&#x27; on line 96 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>        messages = prompt.to_messages()
        if with_functions:
            predicted_message = self.llm.invoke(
                messages,
                functions=self.functions,
                callbacks=callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="203">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_openai_functions_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:287</div>
                    <div class="description">Function &#x27;create_openai_functions_agent&#x27; on line 287 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_openai_functions_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM02: Insecure Output Handling" data-index="204">
                    <div class="finding-header">
                        <span class="finding-title">LLM output used in dangerous sql_injection sink</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM02: Insecure Output Handling</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py:228</div>
                    <div class="description">LLM output from &#x27;self.llm.invoke&#x27; is used in &#x27;SELECT&#x27; on line 228 without sanitization. This creates a sql_injection vulnerability where malicious LLM output can compromise application security.</div>
                    <div class="code-block"><code>        messages = prompt.to_messages()
        predicted_message = self.llm.invoke(
            messages,
            functions=self.functions,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="205">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py:204</div>
                    <div class="description">Function &#x27;plan&#x27; on line 204 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def plan(
        self,
        intermediate_steps: list[tuple[AgentAction, str]],
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; list[AgentAction] | AgentFinish:
        &quot;&quot;&quot;Given input, decided what to do.

        Args:
            intermediate_steps: Steps the LLM has taken to date,
                along with observations.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="206">
                    <div class="finding-header">
                        <span class="finding-title">LLM tool calling without permission checks in &#x27;plan&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py:228</div>
                    <div class="description">Function &#x27;plan&#x27; on line 204 enables LLM tool/function calling without implementing permission checks or authorization. This allows the LLM to autonomously execute tools without human oversight, potentially performing unauthorized or harmful actions.</div>
                    <div class="code-block"><code>        prompt = self.prompt.format_prompt(**full_inputs)
        messages = prompt.to_messages()
        predicted_message = self.llm.invoke(
            messages,
            functions=self.functions,
            callbacks=callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="207">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</div>
                    <div class="description">Function &#x27;create_tool_calling_agent&#x27; on line 18 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_tool_calling_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,
    *,
    message_formatter: MessageFormatter = format_to_tool_messages,
) -&gt; Runnable:
    &quot;&quot;&quot;Create an agent that uses tools.

    Args:
        llm: LLM to use as the agent.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="208">
                    <div class="finding-header">
                        <span class="finding-title">Dynamic tool/plugin loading</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</div>
                    <div class="description">Dynamic tool calling without validation on line 18.</div>
                    <div class="code-block"><code>

def create_tool_calling_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="209">
                    <div class="finding-header">
                        <span class="finding-title">Dynamic tool/plugin loading</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:44</div>
                    <div class="description">Dynamic tool calling without validation on line 44.</div>
                    <div class="code-block"><code>        from langchain_classic.agents import (
            AgentExecutor,
            create_tool_calling_agent,
            tool,
        )
        from langchain_anthropic import ChatAnthropic</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="210">
                    <div class="finding-header">
                        <span class="finding-title">Dynamic tool/plugin loading</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:67</div>
                    <div class="description">Dynamic tool calling without validation on line 67.</div>
                    <div class="code-block"><code>        tools = [magic_function]

        agent = create_tool_calling_agent(model, tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

        agent_executor.invoke({&quot;input&quot;: &quot;what is the value of magic_function(3)?&quot;})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="211">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_tool_calling_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</div>
                    <div class="description">Function &#x27;create_tool_calling_agent&#x27; on line 18 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_tool_calling_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="212">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_structured_chat_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/structured_chat/base.py:166</div>
                    <div class="description">Function &#x27;create_structured_chat_agent&#x27; on line 166 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_structured_chat_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="213">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/json_chat/base.py:14</div>
                    <div class="description">Function &#x27;create_json_chat_agent&#x27; on line 14 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_json_chat_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,
    stop_sequence: bool | list[str] = True,  # noqa: FBT001,FBT002
    tools_renderer: ToolsRenderer = render_text_description,
    template_tool_response: str = TEMPLATE_TOOL_RESPONSE,
) -&gt; Runnable:
    r&quot;&quot;&quot;Create an agent that uses JSON to format its logic, build for Chat Models.

    Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="214">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_json_chat_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/json_chat/base.py:14</div>
                    <div class="description">Function &#x27;create_json_chat_agent&#x27; on line 14 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_json_chat_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: ChatPromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="215">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/xml/base.py:115</div>
                    <div class="description">Function &#x27;create_xml_agent&#x27; on line 115 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_xml_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,
    tools_renderer: ToolsRenderer = render_text_description,
    *,
    stop_sequence: bool | list[str] = True,
) -&gt; Runnable:
    r&quot;&quot;&quot;Create an agent that uses XML to format its logic.

    Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="216">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_xml_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/xml/base.py:115</div>
                    <div class="description">Function &#x27;create_xml_agent&#x27; on line 115 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_xml_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="217">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:74</div>
                    <div class="description">Function &#x27;_get_openai_client&#x27; on line 74 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _get_openai_client() -&gt; openai.OpenAI:
    try:
        import openai

        return openai.OpenAI()
    except ImportError as e:
        msg = &quot;Unable to import openai, please install with `pip install openai`.&quot;
        raise ImportError(msg) from e
    except AttributeError as e:
        msg = (
            &quot;Please make sure you are using a v1.1-compatible version of openai. You &quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="218">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:90</div>
                    <div class="description">Function &#x27;_get_openai_async_client&#x27; on line 90 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _get_openai_async_client() -&gt; openai.AsyncOpenAI:
    try:
        import openai

        return openai.AsyncOpenAI()
    except ImportError as e:
        msg = &quot;Unable to import openai, please install with `pip install openai`.&quot;
        raise ImportError(msg) from e
    except AttributeError as e:
        msg = (
            &quot;Please make sure you are using a v1.1-compatible version of openai. You &quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="219">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:589</div>
                    <div class="description">Function &#x27;_get_response&#x27; on line 589 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_response(self, run: Any) -&gt; Any:
        # TODO: Pagination

        if run.status == &quot;completed&quot;:
            import openai

            major_version = int(openai.version.VERSION.split(&quot;.&quot;)[0])
            minor_version = int(openai.version.VERSION.split(&quot;.&quot;)[1])
            version_gte_1_14 = (major_version &gt; 1) or (
                major_version == 1 and minor_version &gt;= 14  # noqa: PLR2004
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="220">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_get_response&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:589</div>
                    <div class="description">Function &#x27;_get_response&#x27; on line 589 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        )

    def _get_response(self, run: Any) -&gt; Any:
        # TODO: Pagination

        if run.status == &quot;completed&quot;:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="221">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/react/agent.py:16</div>
                    <div class="description">Function &#x27;create_react_agent&#x27; on line 16 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_react_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,
    output_parser: AgentOutputParser | None = None,
    tools_renderer: ToolsRenderer = render_text_description,
    *,
    stop_sequence: bool | list[str] = True,
) -&gt; Runnable:
    r&quot;&quot;&quot;Create an agent that uses ReAct prompting.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="222">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;create_react_agent&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/react/agent.py:16</div>
                    <div class="description">Function &#x27;create_react_agent&#x27; on line 16 makes critical security decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def create_react_agent(
    llm: BaseLanguageModel,
    tools: Sequence[BaseTool],
    prompt: BasePromptTemplate,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="223">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</div>
                    <div class="description">Function &#x27;_run_llm&#x27; on line 861 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _run_llm(
    llm: BaseLanguageModel,
    inputs: dict[str, Any],
    callbacks: Callbacks,
    *,
    tags: list[str] | None = None,
    input_mapper: Callable[[dict], Any] | None = None,
    metadata: dict[str, Any] | None = None,
) -&gt; str | BaseMessage:
    &quot;&quot;&quot;Run the language model on the example.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="224">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:929</div>
                    <div class="description">Function &#x27;_run_chain&#x27; on line 929 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _run_chain(
    chain: Chain | Runnable,
    inputs: dict[str, Any],
    callbacks: Callbacks,
    *,
    tags: list[str] | None = None,
    input_mapper: Callable[[dict], Any] | None = None,
    metadata: dict[str, Any] | None = None,
) -&gt; dict | str:
    &quot;&quot;&quot;Run a chain on inputs.&quot;&quot;&quot;
    inputs_ = inputs if input_mapper is None else input_mapper(inputs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="225">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1512</div>
                    <div class="description">Function &#x27;run_on_dataset&#x27; on line 1512 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def run_on_dataset(
    client: Client | None,
    dataset_name: str,
    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,
    *,
    evaluation: smith_eval.RunEvalConfig | None = None,
    dataset_version: datetime | str | None = None,
    concurrency_level: int = 5,
    project_name: str | None = None,
    project_metadata: dict[str, Any] | None = None,
    verbose: bool = False,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM07: Insecure Plugin Design" data-index="226">
                    <div class="finding-header">
                        <span class="finding-title">Insecure tool function &#x27;_run_llm&#x27; executes dangerous operations</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM07: Insecure Plugin Design</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</div>
                    <div class="description">Tool function &#x27;_run_llm&#x27; on line 861 takes LLM output as a parameter and performs dangerous operations (file_access) without proper validation. Attackers can craft malicious LLM outputs to execute arbitrary commands, access files, or perform SQL injection.</div>
                    <div class="code-block"><code>
## Sync Utilities


def _run_llm(
    llm: BaseLanguageModel,
    inputs: dict[str, Any],
    callbacks: Callbacks,
    *,
    tags: list[str] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="227">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_run_chain&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:929</div>
                    <div class="description">Function &#x27;_run_chain&#x27; on line 929 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def _run_chain(
    chain: Chain | Runnable,
    inputs: dict[str, Any],
    callbacks: Callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="228">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_run_llm&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</div>
                    <div class="description">Function &#x27;_run_llm&#x27; on line 861 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _run_llm(
    llm: BaseLanguageModel,
    inputs: dict[str, Any],
    callbacks: Callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="229">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;run_on_dataset&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1512</div>
                    <div class="description">Function &#x27;run_on_dataset&#x27; on line 1512 makes critical security, data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def run_on_dataset(
    client: Client | None,
    dataset_name: str,
    llm_or_chain_factory: MODEL_OR_CHAIN_FACTORY,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="230">
                    <div class="finding-header">
                        <span class="finding-title">Automated action without confidence threshold in &#x27;_run_chain&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:929</div>
                    <div class="description">Function &#x27;_run_chain&#x27; on line 929 automatically executes actions based on LLM output without checking confidence thresholds or validating output. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _run_chain(
    chain: Chain | Runnable,
    inputs: dict[str, Any],
    callbacks: Callbacks,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="231">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:95</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.reranker.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        &quot;&quot;&quot;Filter down documents based on their relevance to the query.&quot;&quot;&quot;
        results = self.reranker.invoke(
            {&quot;documents&quot;: documents, &quot;query&quot;: query},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="232">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:88</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 88 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -&gt; Sequence[Document]:
        &quot;&quot;&quot;Filter down documents based on their relevance to the query.&quot;&quot;&quot;
        results = self.reranker.invoke(
            {&quot;documents&quot;: documents, &quot;query&quot;: query},
            config={&quot;callbacks&quot;: callbacks},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="233">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:102</div>
                    <div class="description">Function &#x27;from_llm&#x27; on line 102 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def from_llm(
        cls,
        llm: BaseLanguageModel,
        *,
        prompt: BasePromptTemplate | None = None,
        **kwargs: Any,
    ) -&gt; &quot;LLMListwiseRerank&quot;:
        &quot;&quot;&quot;Create a LLMListwiseRerank document compressor from a language model.

        Args:
            llm: The language model to use for filtering. **Must implement</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="234">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py:31</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 31 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -&gt; Sequence[Document]:
        &quot;&quot;&quot;Rerank documents using CrossEncoder.

        Args:
            documents: A sequence of documents to compress.
            query: The query to use for compressing the documents.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="235">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;compress_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py:31</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 31 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="236">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/chain_extract.py:68</div>
                    <div class="description">Function &#x27;compress_documents&#x27; on line 68 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def compress_documents(
        self,
        documents: Sequence[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -&gt; Sequence[Document]:
        &quot;&quot;&quot;Compress page content of raw documents.&quot;&quot;&quot;
        compressed_docs = []
        for doc in documents:
            _input = self.get_input(query, doc)
            output_ = self.llm_chain.invoke(_input, config={&quot;callbacks&quot;: callbacks})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="237">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/self_query/base.py:316</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;self.query_constructor.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; list[Document]:
        structured_query = self.query_constructor.invoke(
            {&quot;query&quot;: query},</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="238">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/self_query/base.py:310</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 310 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -&gt; list[Document]:
        structured_query = self.query_constructor.invoke(
            {&quot;query&quot;: query},
            config={&quot;callbacks&quot;: run_manager.get_child()},
        )
        if self.verbose:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="239">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/self_query/base.py:310</div>
                    <div class="description">Function &#x27;_get_relevant_documents&#x27; on line 310 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def _get_relevant_documents(
        self,
        query: str,
        *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="240">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:402</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.generate_prompt&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                &quot;ChatGeneration&quot;,
                self.generate_prompt(
                    [self._convert_input(input)],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="241">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:492</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                &quot;AIMessageChunk&quot;,
                self.invoke(input, config=config, stop=stop, **kwargs),
            )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="242">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:389</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 389 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; AIMessage:
        config = ensure_config(config)
        return cast(
            &quot;AIMessage&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="243">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:1113</div>
                    <div class="description">Function &#x27;generate_prompt&#x27; on line 1113 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate_prompt(
        self,
        prompts: list[PromptValue],
        stop: list[str] | None = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -&gt; LLMResult:
        prompt_messages = [p.to_messages() for p in prompts]
        return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)

    @override</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="244">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake.py:106</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; Iterator[str]:
        result = self.invoke(input, config)
        for i_c, c in enumerate(result):</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="245">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake.py:98</div>
                    <div class="description">Function &#x27;stream&#x27; on line 98 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[str]:
        result = self.invoke(input, config)
        for i_c, c in enumerate(result):
            if self.sleep is not None:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="246">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake.py:98</div>
                    <div class="description">Function &#x27;stream&#x27; on line 98 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def stream(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="247">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:378</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.generate_prompt&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>        return (
            self.generate_prompt(
                [self._convert_input(input)],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="248">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;inputs&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:431</div>
                    <div class="description">User input parameter &#x27;inputs&#x27; is directly passed to LLM API call &#x27;self.generate_prompt&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            try:
                llm_result = self.generate_prompt(
                    [self._convert_input(input_) for input_ in inputs],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="249">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:518</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            # model doesn&#x27;t implement streaming, so use default implementation
            yield self.invoke(input, config=config, stop=stop, **kwargs)
        else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="250">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;prompts&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:788</div>
                    <div class="description">User input &#x27;prompts&#x27; flows to LLM call via assignment in variable &#x27;prompt_strings&#x27;. Function &#x27;generate_prompt&#x27; may be vulnerable to prompt injection attacks.</div>
                    <div class="code-block"><code>    ) -&gt; LLMResult:
        prompt_strings = [p.to_string() for p in prompts]
        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="251">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:368</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 368 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; str:
        config = ensure_config(config)
        return (
            self.generate_prompt(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="252">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:415</div>
                    <div class="description">Function &#x27;batch&#x27; on line 415 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch(
        self,
        inputs: list[LanguageModelInput],
        config: RunnableConfig | list[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any,
    ) -&gt; list[str]:
        if not inputs:
            return []
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="253">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:508</div>
                    <div class="description">Function &#x27;stream&#x27; on line 508 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: LanguageModelInput,
        config: RunnableConfig | None = None,
        *,
        stop: list[str] | None = None,
        **kwargs: Any,
    ) -&gt; Iterator[str]:
        if type(self)._stream == BaseLLM._stream:  # noqa: SLF001
            # model doesn&#x27;t implement streaming, so use default implementation
            yield self.invoke(input, config=config, stop=stop, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="254">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:781</div>
                    <div class="description">Function &#x27;generate_prompt&#x27; on line 781 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def generate_prompt(
        self,
        prompts: list[PromptValue],
        stop: list[str] | None = None,
        callbacks: Callbacks | list[Callbacks] | None = None,
        **kwargs: Any,
    ) -&gt; LLMResult:
        prompt_strings = [p.to_string() for p in prompts]
        return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)

    @override</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="255">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake_chat_models.py:158</div>
                    <div class="description">Function &#x27;batch&#x27; on line 158 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch(
        self,
        inputs: list[Any],
        config: RunnableConfig | list[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any,
    ) -&gt; list[AIMessage]:
        if isinstance(config, list):
            return [
                self.invoke(m, c, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="256">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake_chat_models.py:158</div>
                    <div class="description">Function &#x27;batch&#x27; on line 158 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>    @override
    # manually override batch to preserve batch ordering with no concurrency
    def batch(
        self,
        inputs: list[Any],
        config: RunnableConfig | list[RunnableConfig] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="257">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:76</div>
                    <div class="description">Function &#x27;tool&#x27; on line 76 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def tool(
    name_or_callable: str | Callable | None = None,
    runnable: Runnable | None = None,
    *args: Any,
    description: str | None = None,
    return_direct: bool = False,
    args_schema: ArgsSchema | None = None,
    infer_schema: bool = True,
    response_format: Literal[&quot;content&quot;, &quot;content_and_artifact&quot;] = &quot;content&quot;,
    parse_docstring: bool = False,
    error_on_invalid_docstring: bool = True,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="258">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:413</div>
                    <div class="description">Function &#x27;convert_runnable_to_tool&#x27; on line 413 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def convert_runnable_to_tool(
    runnable: Runnable,
    args_schema: type[BaseModel] | None = None,
    *,
    name: str | None = None,
    description: str | None = None,
    arg_types: dict[str, type] | None = None,
) -&gt; BaseTool:
    &quot;&quot;&quot;Convert a Runnable into a BaseTool.

    Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="259">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:253</div>
                    <div class="description">Function &#x27;_create_tool_factory&#x27; on line 253 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _create_tool_factory(
        tool_name: str,
    ) -&gt; Callable[[Callable | Runnable], BaseTool]:
        &quot;&quot;&quot;Create a decorator that takes a callable and returns a tool.

        Args:
            tool_name: The name that will be assigned to the tool.

        Returns:
            A function that takes a callable or Runnable and returns a tool.
        &quot;&quot;&quot;</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="260">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:450</div>
                    <div class="description">Function &#x27;invoke_wrapper&#x27; on line 450 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke_wrapper(callbacks: Callbacks | None = None, **kwargs: Any) -&gt; Any:
        return runnable.invoke(kwargs, config={&quot;callbacks&quot;: callbacks})

    if (
        arg_types is None
        and schema.get(&quot;type&quot;) == &quot;object&quot;
        and schema.get(&quot;properties&quot;)
    ):
        args_schema = runnable.input_schema
    else:
        args_schema = _get_schema_from_runnable_and_arg_types(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="261">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:279</div>
                    <div class="description">Function &#x27;invoke_wrapper&#x27; on line 279 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>                def invoke_wrapper(
                    callbacks: Callbacks | None = None, **kwargs: Any
                ) -&gt; Any:
                    return runnable.invoke(kwargs, {&quot;callbacks&quot;: callbacks})

                coroutine = ainvoke_wrapper
                func = invoke_wrapper
                schema: ArgsSchema | None = runnable.input_schema
                tool_description = description or repr(runnable)
            elif inspect.iscoroutinefunction(dec_func):
                coroutine = dec_func</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="262">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;tool&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:76</div>
                    <div class="description">Function &#x27;tool&#x27; on line 76 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def tool(
    name_or_callable: str | Callable | None = None,
    runnable: Runnable | None = None,
    *args: Any,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="263">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;convert_runnable_to_tool&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:413</div>
                    <div class="description">Function &#x27;convert_runnable_to_tool&#x27; on line 413 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def convert_runnable_to_tool(
    runnable: Runnable,
    args_schema: type[BaseModel] | None = None,
    *,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="264">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute/network operation without confirmation in &#x27;_create_tool_factory&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:253</div>
                    <div class="description">Function &#x27;_create_tool_factory&#x27; on line 253 performs high-risk write/execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>    &quot;&quot;&quot;  # noqa: D214, D410, D411  # We&#x27;re intentionally showing bad formatting in examples

    def _create_tool_factory(
        tool_name: str,
    ) -&gt; Callable[[Callable | Runnable], BaseTool]:
        &quot;&quot;&quot;Create a decorator that takes a callable and returns a tool.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="265">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;invoke_wrapper&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:450</div>
                    <div class="description">Function &#x27;invoke_wrapper&#x27; on line 450 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return await runnable.ainvoke(kwargs, config={&quot;callbacks&quot;: callbacks})

    def invoke_wrapper(callbacks: Callbacks | None = None, **kwargs: Any) -&gt; Any:
        return runnable.invoke(kwargs, config={&quot;callbacks&quot;: callbacks})

    if (</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="266">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute operation without confirmation in &#x27;invoke_wrapper&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:279</div>
                    <div class="description">Function &#x27;invoke_wrapper&#x27; on line 279 performs high-risk execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>                    return await runnable.ainvoke(kwargs, {&quot;callbacks&quot;: callbacks})

                def invoke_wrapper(
                    callbacks: Callbacks | None = None, **kwargs: Any
                ) -&gt; Any:
                    return runnable.invoke(kwargs, {&quot;callbacks&quot;: callbacks})</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="267">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;query&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/retriever.py:65</div>
                    <div class="description">User input parameter &#x27;query&#x27; is directly passed to LLM API call &#x27;retriever.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>    ) -&gt; str | tuple[str, list[Document]]:
        docs = retriever.invoke(query, config={&quot;callbacks&quot;: callbacks})
        content = document_separator.join(</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="268">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/retriever.py:31</div>
                    <div class="description">Function &#x27;create_retriever_tool&#x27; on line 31 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def create_retriever_tool(
    retriever: BaseRetriever,
    name: str,
    description: str,
    *,
    document_prompt: BasePromptTemplate | None = None,
    document_separator: str = &quot;\n\n&quot;,
    response_format: Literal[&quot;content&quot;, &quot;content_and_artifact&quot;] = &quot;content&quot;,
) -&gt; StructuredTool:
    r&quot;&quot;&quot;Create a tool to do retrieval of documents.
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="269">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/retriever.py:62</div>
                    <div class="description">Function &#x27;func&#x27; on line 62 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def func(
        query: str, callbacks: Callbacks = None
    ) -&gt; str | tuple[str, list[Document]]:
        docs = retriever.invoke(query, config={&quot;callbacks&quot;: callbacks})
        content = document_separator.join(
            format_document(doc, document_prompt_) for doc in docs
        )
        if response_format == &quot;content_and_artifact&quot;:
            return (content, docs)
        return content
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="270">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/pydantic.py:231</div>
                    <div class="description">Function &#x27;_create_subset_model_v2&#x27; on line 231 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _create_subset_model_v2(
    name: str,
    model: type[BaseModel],
    field_names: list[str],
    *,
    descriptions: dict | None = None,
    fn_description: str | None = None,
) -&gt; type[BaseModel]:
    &quot;&quot;&quot;Create a Pydantic model with a subset of the model fields.&quot;&quot;&quot;
    descriptions_ = descriptions or {}
    fields = {}</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="271">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write operation without confirmation in &#x27;_create_subset_model_v2&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/pydantic.py:231</div>
                    <div class="description">Function &#x27;_create_subset_model_v2&#x27; on line 231 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def _create_subset_model_v2(
    name: str,
    model: type[BaseModel],
    field_names: list[str],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="272">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/function_calling.py:193</div>
                    <div class="description">Function &#x27;_convert_python_function_to_openai_function&#x27; on line 193 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>def _convert_python_function_to_openai_function(
    function: Callable,
) -&gt; FunctionDescription:
    &quot;&quot;&quot;Convert a Python function to an OpenAI function-calling API compatible dict.

    Assumes the Python function has type hints and a docstring with a description. If
        the docstring has Google Python style argument descriptions, these will be
        included as well.

    Args:
        function: The Python function to convert.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM08: Excessive Agency" data-index="273">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write operation without confirmation in &#x27;_convert_python_function_to_openai_function&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/function_calling.py:193</div>
                    <div class="description">Function &#x27;_convert_python_function_to_openai_function&#x27; on line 193 performs high-risk write operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>

def _convert_python_function_to_openai_function(
    function: Callable,
) -&gt; FunctionDescription:
    &quot;&quot;&quot;Convert a Python function to an OpenAI function-calling API compatible dict.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-info finding-hidden" data-severity="info" data-category="LLM09: Overreliance" data-index="274">
                    <div class="finding-header">
                        <span class="finding-title">Critical decision without oversight in &#x27;_convert_pydantic_to_openai_function&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM09: Overreliance</span>
                            <span class="badge badge-info">INFO</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/function_calling.py:153</div>
                    <div class="description">Function &#x27;_convert_pydantic_to_openai_function&#x27; on line 153 makes critical data_modification decisions based on LLM output without human oversight or verification. No action edges detected - advisory only.</div>
                    <div class="code-block"><code>

def _convert_pydantic_to_openai_function(
    model: type,
    *,
    name: str | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM05: Supply Chain Vulnerabilities" data-index="275">
                    <div class="finding-header">
                        <span class="finding-title">Network fetch combined with code execution</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/graph_mermaid.py:44</div>
                    <div class="description">This file downloads external content (lines [456, 481]) and executes code (lines [44, 356, 363]). This pattern enables remote code execution attacks if the fetched content is not properly validated.</div>
                    <div class="code-block"><code>    _HAS_PYPPETEER = False

MARKDOWN_SPECIAL_CHARS = &quot;*_`&quot;

_HEX_COLOR_PATTERN = re.compile(r&quot;^#(?:[0-9a-fA-F]{3}){1,2}$&quot;)


def draw_mermaid(
    nodes: dict[str, Node],
    edges: list[Edge],</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="276">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:189</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;bound.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                return bound.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="277">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:185</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;bound.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    return bound.invoke(input_, config, **kwargs)
                except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="278">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:142</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 142 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        runnable, config = self.prepare(config)
        return runnable.invoke(input, config, **kwargs)

    @override
    async def ainvoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        runnable, config = self.prepare(config)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="279">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:156</div>
                    <div class="description">Function &#x27;batch&#x27; on line 156 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch(
        self,
        inputs: list[Input],
        config: RunnableConfig | list[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any | None,
    ) -&gt; list[Output]:
        configs = get_config_list(config, len(inputs))
        prepared = [self.prepare(c) for c in configs]
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="280">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:178</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 178 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(
            prepared: tuple[Runnable[Input, Output], RunnableConfig],
            input_: Input,
        ) -&gt; Output | Exception:
            bound, config = prepared
            if return_exceptions:
                try:
                    return bound.invoke(input_, config, **kwargs)
                except Exception as e:
                    return e
            else:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="281">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:142</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 142 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        runnable, config = self.prepare(config)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="282">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:156</div>
                    <div class="description">Function &#x27;batch&#x27; on line 156 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def batch(
        self,
        inputs: list[Input],
        config: RunnableConfig | list[RunnableConfig] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="283">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:178</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 178 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            return []

        def invoke(
            prepared: tuple[Runnable[Input, Output], RunnableConfig],
            input_: Input,
        ) -&gt; Output | Exception:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="284">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:215</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;condition.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
                expression_value = condition.invoke(
                    input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="285">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:234</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;self.default.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                output = self.default.invoke(
                    input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="286">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:224</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;runnable.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                if expression_value:
                    output = runnable.invoke(
                        input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="287">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:327</div>
                    <div class="description">User input parameter &#x27;input&#x27; is directly passed to LLM API call &#x27;condition.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>
                expression_value = condition.invoke(
                    input,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="288">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:189</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 189 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        &quot;&quot;&quot;First evaluates the condition, then delegate to `True` or `False` branch.

        Args:
            input: The input to the `Runnable`.
            config: The configuration for the `Runnable`.
            **kwargs: Additional keyword arguments to pass to the `Runnable`.

        Returns:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="289">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:296</div>
                    <div class="description">Function &#x27;stream&#x27; on line 296 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Iterator[Output]:
        &quot;&quot;&quot;First evaluates the condition, then delegate to `True` or `False` branch.

        Args:
            input: The input to the `Runnable`.
            config: The configuration for the `Runnable`.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="290">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:189</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 189 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def invoke(
        self, input: Input, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        &quot;&quot;&quot;First evaluates the condition, then delegate to `True` or `False` branch.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="291">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:296</div>
                    <div class="description">Function &#x27;stream&#x27; on line 296 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="292">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/retry.py:188</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;super().invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            with attempt:
                result = super().invoke(
                    input_,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="293">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/retry.py:179</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 179 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _invoke(
        self,
        input_: Input,
        run_manager: &quot;CallbackManagerForChainRun&quot;,
        config: RunnableConfig,
        **kwargs: Any,
    ) -&gt; Output:
        for attempt in self._sync_retrying(reraise=True):
            with attempt:
                result = super().invoke(
                    input_,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="294">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/retry.py:179</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 179 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        ]

    def _invoke(
        self,
        input_: Input,
        run_manager: &quot;CallbackManagerForChainRun&quot;,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="295">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:162</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;runnable.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                return runnable.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="296">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:158</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;runnable.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    return runnable.invoke(input_, config, **kwargs)
                except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="297">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:107</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 107 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self, input: RouterInput, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        key = input[&quot;key&quot;]
        actual_input = input[&quot;input&quot;]
        if key not in self.runnables:
            msg = f&quot;No runnable associated with key &#x27;{key}&#x27;&quot;
            raise ValueError(msg)

        runnable = self.runnables[key]
        return runnable.invoke(actual_input, config)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="298">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:136</div>
                    <div class="description">Function &#x27;batch&#x27; on line 136 has 4 DoS risk(s): No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch(
        self,
        inputs: list[RouterInput],
        config: RunnableConfig | list[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any | None,
    ) -&gt; list[Output]:
        if not inputs:
            return []
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="299">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:153</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 153 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(
            runnable: Runnable[Input, Output], input_: Input, config: RunnableConfig
        ) -&gt; Output | Exception:
            if return_exceptions:
                try:
                    return runnable.invoke(input_, config, **kwargs)
                except Exception as e:
                    return e
            else:
                return runnable.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="300">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:107</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 107 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def invoke(
        self, input: RouterInput, config: RunnableConfig | None = None, **kwargs: Any
    ) -&gt; Output:
        key = input[&quot;key&quot;]</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="301">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:136</div>
                    <div class="description">Function &#x27;batch&#x27; on line 136 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def batch(
        self,
        inputs: list[RouterInput],
        config: RunnableConfig | list[RunnableConfig] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="302">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:153</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 153 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            raise ValueError(msg)

        def invoke(
            runnable: Runnable[Input, Output], input_: Input, config: RunnableConfig
        ) -&gt; Output | Exception:
            if return_exceptions:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="303">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/passthrough.py:480</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 480 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _invoke(
        self,
        value: dict[str, Any],
        run_manager: CallbackManagerForChainRun,
        config: RunnableConfig,
        **kwargs: Any,
    ) -&gt; dict[str, Any]:
        if not isinstance(value, dict):
            msg = &quot;The input to RunnablePassthrough.assign() must be a dict.&quot;
            raise ValueError(msg)  # noqa: TRY004
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="304">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/passthrough.py:480</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 480 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return graph

    def _invoke(
        self,
        value: dict[str, Any],
        run_manager: CallbackManagerForChainRun,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="305">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:979</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>            else:
                out = self.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM01: Prompt Injection" data-index="306">
                    <div class="finding-header">
                        <span class="finding-title">User input &#x27;input_&#x27; embedded in LLM prompt</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM01: Prompt Injection</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:975</div>
                    <div class="description">User input parameter &#x27;input_&#x27; is directly passed to LLM API call &#x27;self.invoke&#x27;. This is a high-confidence prompt injection vector.</div>
                    <div class="code-block"><code>                try:
                    out: Output | Exception = self.invoke(input_, config, **kwargs)
                except Exception as e:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="307">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:867</div>
                    <div class="description">Function &#x27;batch&#x27; on line 867 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch(
        self,
        inputs: list[Input],
        config: RunnableConfig | list[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any | None,
    ) -&gt; list[Output]:
        &quot;&quot;&quot;Default implementation runs invoke in parallel using a thread pool executor.

        The default implementation of batch works well for IO bound runnables.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="308">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:937</div>
                    <div class="description">Function &#x27;batch_as_completed&#x27; on line 937 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def batch_as_completed(
        self,
        inputs: Sequence[Input],
        config: RunnableConfig | Sequence[RunnableConfig] | None = None,
        *,
        return_exceptions: bool = False,
        **kwargs: Any | None,
    ) -&gt; Iterator[tuple[int, Output | Exception]]:
        &quot;&quot;&quot;Run `invoke` in parallel on a list of inputs.

        Yields results as they complete.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="309">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:1130</div>
                    <div class="description">Function &#x27;stream&#x27; on line 1130 has 5 DoS risk(s): No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Iterator[Output]:
        &quot;&quot;&quot;Default implementation of `stream`, which calls `invoke`.

        Subclasses must override this method if they support streaming output.

        Args:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="310">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:4847</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 4847 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def _invoke(
        self,
        input_: Input,
        run_manager: CallbackManagerForChainRun,
        config: RunnableConfig,
        **kwargs: Any,
    ) -&gt; Output:
        if inspect.isgeneratorfunction(self.func):
            output: Output | None = None
            for chunk in call_func_with_variable_args(
                cast(&quot;Callable[[Input], Iterator[Output]]&quot;, self.func),</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="311">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:5685</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 5685 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def invoke(
        self,
        input: Input,
        config: RunnableConfig | None = None,
        **kwargs: Any | None,
    ) -&gt; Output:
        return self.bound.invoke(
            input,
            self._merge_configs(config),
            **{**self.kwargs, **kwargs},
        )</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="312">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:901</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 901 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(input_: Input, config: RunnableConfig) -&gt; Output | Exception:
            if return_exceptions:
                try:
                    return self.invoke(input_, config, **kwargs)
                except Exception as e:
                    return e
            else:
                return self.invoke(input_, config, **kwargs)

        # If there&#x27;s only one input, don&#x27;t bother with the executor
        if len(inputs) == 1:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-critical finding-hidden" data-severity="critical" data-category="LLM04: Model Denial of Service" data-index="313">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-critical">CRITICAL</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:970</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 970 has 4 DoS risk(s): No rate limiting, No input length validation, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>        def invoke(
            i: int, input_: Input, config: RunnableConfig
        ) -&gt; tuple[int, Output | Exception]:
            if return_exceptions:
                try:
                    out: Output | Exception = self.invoke(input_, config, **kwargs)
                except Exception as e:
                    out = e
            else:
                out = self.invoke(input_, config, **kwargs)
</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="314">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:867</div>
                    <div class="description">Function &#x27;batch&#x27; on line 867 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return await run_in_executor(config, self.invoke, input, config, **kwargs)

    def batch(
        self,
        inputs: list[Input],
        config: RunnableConfig | list[RunnableConfig] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="315">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;batch_as_completed&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:937</div>
                    <div class="description">Function &#x27;batch_as_completed&#x27; on line 937 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>    ) -&gt; Iterator[tuple[int, Output | Exception]]: ...

    def batch_as_completed(
        self,
        inputs: Sequence[Input],
        config: RunnableConfig | Sequence[RunnableConfig] | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="316">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;stream&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:1130</div>
                    <div class="description">Function &#x27;stream&#x27; on line 1130 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>            yield await coro

    def stream(
        self,
        input: Input,
        config: RunnableConfig | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="317">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;_invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:4847</div>
                    <div class="description">Function &#x27;_invoke&#x27; on line 4847 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        return self._repr

    def _invoke(
        self,
        input_: Input,
        run_manager: CallbackManagerForChainRun,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="318">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:5685</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 5685 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>
    @override
    def invoke(
        self,
        input: Input,
        config: RunnableConfig | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="319">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:901</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 901 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        configs = get_config_list(config, len(inputs))

        def invoke(input_: Input, config: RunnableConfig) -&gt; Output | Exception:
            if return_exceptions:
                try:
                    return self.invoke(input_, config, **kwargs)</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="320">
                    <div class="finding-header">
                        <span class="finding-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:970</div>
                    <div class="description">Function &#x27;invoke&#x27; on line 970 performs high-risk execute/network operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>        configs = get_config_list(config, len(inputs))

        def invoke(
            i: int, input_: Input, config: RunnableConfig
        ) -&gt; tuple[int, Output | Exception]:
            if return_exceptions:</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                <div class="finding severity-medium finding-hidden" data-severity="medium" data-category="LLM05: Supply Chain Vulnerabilities" data-index="321">
                    <div class="finding-header">
                        <span class="finding-title">Code execution on external content</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM05: Supply Chain Vulnerabilities</span>
                            <span class="badge badge-medium">MEDIUM</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/documents/base.py:7</div>
                    <div class="description">eval() on non-literal content on line 7. </div>
                    <div class="code-block"><code>- `BaseMedia`: Base class providing `id` and `metadata` fields
- `Blob`: Raw data loading (files, binary data) - used by document loaders
- `Document`: Text content for retrieval (RAG, vector stores, semantic search)

!!! note &quot;Not for LLM chat messages&quot;
    These classes are for data processing pipelines, not LLM I/O. For multimodal</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Secure Code Execution:
1. NEVER use eval/exec on untrusted input
2. Use safe alternatives (json.loads, ast.literal_eval)
3. Validate and sanitize all external content
4. Use sandboxed execution environments
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM04: Model Denial of Service" data-index="322">
                    <div class="finding-header">
                        <span class="finding-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM04: Model Denial of Service</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tracers/evaluation.py:64</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 64 has 3 DoS risk(s): No rate limiting, No timeout configuration, No token/context limits. These missing protections enable attackers to exhaust model resources through excessive requests, large inputs, or recursive calls, leading to service degradation or unavailability.</div>
                    <div class="code-block"><code>    def __init__(
        self,
        evaluators: Sequence[langsmith.RunEvaluator],
        client: langsmith.Client | None = None,
        example_id: UUID | str | None = None,
        skip_unfinished: bool = True,  # noqa: FBT001,FBT002
        project_name: str | None = &quot;evaluators&quot;,
        max_concurrency: int | None = None,
        **kwargs: Any,
    ) -&gt; None:
        &quot;&quot;&quot;Create an EvaluatorCallbackHandler.</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets
                    </div>
                </div>
                
                <div class="finding severity-high finding-hidden" data-severity="high" data-category="LLM08: Excessive Agency" data-index="323">
                    <div class="finding-header">
                        <span class="finding-title">High-risk write/execute operation without confirmation in &#x27;__init__&#x27;</span>
                        <div>
                            <span class="badge" style="background: #6c757d; margin-right: 5px;">LLM08: Excessive Agency</span>
                            <span class="badge badge-high">HIGH</span>
                        </div>
                    </div>
                    <div class="location">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tracers/evaluation.py:64</div>
                    <div class="description">Function &#x27;__init__&#x27; on line 64 performs high-risk write/execute operations based on LLM decisions without requiring user confirmation or approval. This allows the LLM to autonomously execute potentially destructive or sensitive actions.</div>
                    <div class="code-block"><code>    lock: threading.Lock

    def __init__(
        self,
        evaluators: Sequence[langsmith.RunEvaluator],
        client: langsmith.Client | None = None,</code></div>
                    <div class="remediation">
                        <div class="remediation-title">Remediation</div>
                        High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits
                    </div>
                </div>
                
                </div>

                <!-- Load More Button -->
                <div id="findings-load-more-container" style="text-align: center; margin-top: 16px;">
                    <button id="findings-load-more-btn" class="load-more-btn" onclick="loadMoreFindings()">
                        Show More <span id="findings-remaining-count"></span>
                    </button>
                </div>
            </div>
            
        </div>
        
            <div id="security-posture" class="tab-content">
                
        <div class="audit-summary">
            <div class="audit-metric">
                <div class="audit-metric-value">19</div>
                <div class="audit-metric-label">Overall Score</div>
                <span class="maturity-badge maturity-initial">Initial</span>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">25</div>
                <div class="audit-metric-label">Controls Detected</div>
                <span class="maturity-badge" style="background: #f1f5f9; color: #64748b;">of 61</span>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">2660</div>
                <div class="audit-metric-label">Files Analyzed</div>
            </div>
            <div class="audit-metric">
                <div class="audit-metric-value">386</div>
                <div class="audit-metric-label">Total Recommendations</div>
            </div>
        </div>

        <div class="section-header">
            <h3 style="color: var(--text-primary); border-bottom: 2px solid var(--accent-primary); padding-bottom: 8px;">Category Scores</h3>
            <button class="expand-toggle" id="toggle-categories" onclick="toggleAllCategories()">Expand All</button>
        </div>
        <div class="category-grid">
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Prompt Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">28/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 28.125%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Prompt Sanitization</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Rate Limiting</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Input Validation</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Output Filtering</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Context Window Protection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Red Team Testing</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Prompt Anomaly Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">System Prompt Protection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Model Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">25/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 25.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Access Control</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Versioning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Dependency Scanning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">API Security</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Source Verification</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Differential Privacy</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Watermarking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Secure Model Loading</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Data Privacy</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">31/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 31.25%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">PII Detection</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Redaction</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Encryption</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Audit Logging</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Consent Management</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">NER PII Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Retention Policy</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">GDPR Compliance</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 4 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 4 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">OWASP LLM Top 10</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">45/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 45.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">LLM01: Prompt Injection Defense</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM02: Insecure Output Handling</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM03: Training Data Poisoning</span>
                <span class="control-status status-partial">Partial</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM04: Model Denial of Service</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM05: Supply Chain Vulnerabilities</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM06: Sensitive Information Disclosure</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM07: Insecure Plugin Design</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM08: Excessive Agency</span>
                <span class="control-status status-partial">Partial</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM09: Overreliance</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">LLM10: Model Theft</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 6 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 2 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Blue Team Operations</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">21/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 21.428571428571427%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Model Monitoring</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Drift Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Anomaly Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Adversarial Attack Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">AI Incident Response</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Drift Monitoring</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Data Quality Monitoring</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 2 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">AI Governance</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">0/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 0.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Model Explainability</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Bias Detection</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Documentation</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Compliance Tracking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Human Oversight</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 0 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 5 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Supply Chain Security</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">25/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 25.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Dependency Scanning</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Provenance Tracking</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Integrity Verification</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 1 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Hallucination Mitigation</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">35/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 35.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">RAG Implementation</span>
                <span class="control-status status-detected">Advanced</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Confidence Scoring</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Source Attribution</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Temperature Control</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Fact Checking</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 3 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 2 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Ethical AI &amp; Bias</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">12/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 12.5%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Fairness Metrics</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Explainability</span>
                <span class="control-status status-detected">Intermediate</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Bias Testing</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Model Cards</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 1 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        
        <div class="category-card">
            <div class="category-header" onclick="toggleCategory(this)">
                <div class="category-header-left">
                    <span class="category-name">Incident Response</span>
                </div>
                <div style="display: flex; align-items: center; gap: 12px;">
                    <span class="category-score">0/100</span>
                    <svg class="accordion-icon" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
        </svg>
                </div>
            </div>
            <div class="category-progress">
                <div class="category-progress-fill" style="width: 0.0%"></div>
            </div>
            <div class="category-content">
                <ul class="control-list">
                    
            <li class="control-item">
                <span class="control-name">Monitoring Integration</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Audit Logging</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
            <li class="control-item">
                <span class="control-name">Rollback Capability</span>
                <span class="control-status status-missing">Missing</span>
            </li>
            
                </ul>
                <div class="category-stats">
                    <span class="stat-item"><span class="stat-dot detected"></span> 0 Detected</span>
                    <span class="stat-item"><span class="stat-dot partial"></span> 0 Partial</span>
                    <span class="stat-item"><span class="stat-dot missing"></span> 3 Missing</span>
                </div>
            </div>
        </div>
        </div>
            <div class="recommendations-section">
                <h3>All Recommendations (386)</h3>

                <!-- Severity Filter -->
                <div class="severity-filter" style="margin-bottom: 16px; display: flex; gap: 8px; flex-wrap: wrap;">
                    <button class="severity-filter-btn active" data-severity="all" onclick="filterBySeverity('all')">
                        All <span class="filter-count">(386)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="critical" onclick="filterBySeverity('critical')">
                        Critical <span class="filter-count">(197)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="high" onclick="filterBySeverity('high')">
                        High <span class="filter-count">(134)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="medium" onclick="filterBySeverity('medium')">
                        Medium <span class="filter-count">(23)</span>
                    </button>
                    <button class="severity-filter-btn" data-severity="low" onclick="filterBySeverity('low')">
                        Low <span class="filter-count">(32)</span>
                    </button>
                </div>

                <div class="rec-list" id="recommendations-list">
            
                <div class="rec-item rec-critical " data-priority="critical" data-index="0">
                    <div class="rec-header">
                        <span class="rec-title">Rate Limiting</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="1">
                    <div class="rec-header">
                        <span class="rec-title">Context Window Protection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="2">
                    <div class="rec-header">
                        <span class="rec-title">Red Team Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;ConfigAnalyzer&#x27; object has no attribute &#x27;file_exists&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="3">
                    <div class="rec-header">
                        <span class="rec-title">Prompt Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement statistical analysis on prompt patterns</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="4">
                    <div class="rec-header">
                        <span class="rec-title">Prompt Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use ML-based anomaly detection for unusual inputs</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="5">
                    <div class="rec-header">
                        <span class="rec-title">Prompt Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up alerts for prompt anomaly detection</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="6">
                    <div class="rec-header">
                        <span class="rec-title">System Prompt Protection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="7">
                    <div class="rec-header">
                        <span class="rec-title">Access Control</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="8">
                    <div class="rec-header">
                        <span class="rec-title">Model Versioning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical " data-priority="critical" data-index="9">
                    <div class="rec-header">
                        <span class="rec-title">Dependency Scanning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="10">
                    <div class="rec-header">
                        <span class="rec-title">API Security</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="11">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement watermarking for model outputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="12">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use cryptographic watermarks for model weights</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="13">
                    <div class="rec-header">
                        <span class="rec-title">Model Watermarking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Track watermark verification for model theft detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="14">
                    <div class="rec-header">
                        <span class="rec-title">Consent Management</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="15">
                    <div class="rec-header">
                        <span class="rec-title">NER PII Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Presidio or SpaCy for NER-based PII detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="16">
                    <div class="rec-header">
                        <span class="rec-title">NER PII Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement custom NER models for domain-specific PII</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="17">
                    <div class="rec-header">
                        <span class="rec-title">NER PII Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Run PII detection on all inputs and outputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="18">
                    <div class="rec-header">
                        <span class="rec-title">Data Retention Policy</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="19">
                    <div class="rec-header">
                        <span class="rec-title">GDPR Compliance</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="20">
                    <div class="rec-header">
                        <span class="rec-title">LLM04: Model Denial of Service</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="21">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement rate limiting on API endpoints</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="22">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Add query logging and anomaly detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="23">
                    <div class="rec-header">
                        <span class="rec-title">LLM10: Model Theft</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for extraction patterns</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="24">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement drift detection with evidently or alibi-detect</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="25">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor input data distribution changes</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="26">
                    <div class="rec-header">
                        <span class="rec-title">Drift Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up automated alerts for drift events</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="27">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement anomaly detection on model inputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="28">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor for unusual query patterns</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="29">
                    <div class="rec-header">
                        <span class="rec-title">Anomaly Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use statistical methods or ML-based detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="30">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement adversarial input detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="31">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use adversarial robustness toolkits</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="32">
                    <div class="rec-header">
                        <span class="rec-title">Adversarial Attack Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Add input perturbation analysis</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="33">
                    <div class="rec-header">
                        <span class="rec-title">AI Incident Response</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="34">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Evidently or alibi-detect for drift monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="35">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Set up automated alerts for significant drift</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="36">
                    <div class="rec-header">
                        <span class="rec-title">Model Drift Monitoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement automatic retraining pipelines</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="37">
                    <div class="rec-header">
                        <span class="rec-title">Model Explainability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use SHAP or LIME for model explanations</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="38">
                    <div class="rec-header">
                        <span class="rec-title">Model Explainability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Provide decision explanations in outputs</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="39">
                    <div class="rec-header">
                        <span class="rec-title">Model Explainability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement feature attribution tracking</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="40">
                    <div class="rec-header">
                        <span class="rec-title">Bias Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Fairlearn or AIF360 for bias detection</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="41">
                    <div class="rec-header">
                        <span class="rec-title">Bias Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement fairness metrics tracking</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="42">
                    <div class="rec-header">
                        <span class="rec-title">Bias Detection</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Test for demographic parity and equalized odds</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="43">
                    <div class="rec-header">
                        <span class="rec-title">Model Documentation</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="44">
                    <div class="rec-header">
                        <span class="rec-title">Compliance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="45">
                    <div class="rec-header">
                        <span class="rec-title">Human Oversight</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="46">
                    <div class="rec-header">
                        <span class="rec-title">Dependency Scanning</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="47">
                    <div class="rec-header">
                        <span class="rec-title">Model Provenance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use MLflow, DVC, or Weights &amp; Biases for model tracking</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="48">
                    <div class="rec-header">
                        <span class="rec-title">Model Provenance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement model versioning with metadata</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="49">
                    <div class="rec-header">
                        <span class="rec-title">Model Provenance Tracking</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Maintain model registry with provenance information</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="50">
                    <div class="rec-header">
                        <span class="rec-title">Confidence Scoring</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="51">
                    <div class="rec-header">
                        <span class="rec-title">Temperature Control</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="52">
                    <div class="rec-header">
                        <span class="rec-title">Fairness Metrics</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use Fairlearn or AIF360 for fairness metrics</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="53">
                    <div class="rec-header">
                        <span class="rec-title">Fairness Metrics</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement demographic parity testing</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="54">
                    <div class="rec-header">
                        <span class="rec-title">Fairness Metrics</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Monitor fairness metrics in production</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="55">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement adversarial testing for bias</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="56">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Test across demographic groups</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="57">
                    <div class="rec-header">
                        <span class="rec-title">Bias Testing</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Use TextAttack or CheckList for NLP bias testing</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="58">
                    <div class="rec-header">
                        <span class="rec-title">Model Cards</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;ConfigAnalyzer&#x27; object has no attribute &#x27;file_exists&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="59">
                    <div class="rec-header">
                        <span class="rec-title">Monitoring Integration</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="60">
                    <div class="rec-header">
                        <span class="rec-title">Audit Logging</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="61">
                    <div class="rec-header">
                        <span class="rec-title">Rollback Capability</span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #667eea; color: white;">Audit</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Detection failed: &#x27;bool&#x27; object has no attribute &#x27;lower&#x27;</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="62">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/embeddings/base.py:429</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="63">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:3754</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="64">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1338</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="65">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1714</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="66">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:833</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="67">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:720</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="68">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:812</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="69">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:945</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="70">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;messages&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:752</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="71">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:723</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="72">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1792</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="73">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:589</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="74">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:422</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="75">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:528</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="76">
                    <div class="rec-header">
                        <span class="rec-title">Network fetch combined with code execution<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/groq/langchain_groq/chat_models.py:1383</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="77">
                    <div class="rec-header">
                        <span class="rec-title">Network fetch combined with code execution<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/mistralai/langchain_mistralai/chat_models.py:86</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="78">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/chat_models/base.py:701</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="79">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/chat_models/base.py:695</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="80">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;request&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1108</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="81">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:543</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="82">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:980</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="83">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;create_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:543</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="84">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;request&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:134</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="85">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_emulator.py:150</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="86">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;request&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:288</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="87">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:295</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="88">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:297</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="89">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/summarization.py:562</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="90">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:245</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="91">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:244</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="92">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:281</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="93">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/contextual_compression.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="94">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/contextual_compression.py:27</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="95">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/merger_retriever.py:69</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="96">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/re_phraser.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="97">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/re_phraser.py:61</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="98">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/ensemble.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="99">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/ensemble.py:202</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="100">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:179</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="101">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;question&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:199</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="102">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:164</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="103">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:185</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="104">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/chat_memory.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="105">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore.py:73</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="106">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore.py:67</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="107">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chat_models/base.py:773</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="108">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chat_models/base.py:767</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="109">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:419</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="110">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:531</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="111">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompt_value&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:251</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="112">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:117</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="113">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:245</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="114">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/fix.py:81</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="115">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/example_generator.py:9</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="116">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:117</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="117">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_list&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:241</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="118">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="119">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:120</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="120">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="121">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="122">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="123">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:369</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="124">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;question&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:67</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="125">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;question&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:137</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="126">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;text&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:81</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="127">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:96</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="128">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous command_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:81</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for Command Injection:
1. Never pass LLM output to shell commands
2. Use subprocess with shell=False and list arguments
3. Apply allowlist validation for expected values
4. Use shlex.quote() if shell execution is unavoidable
5. Consider alternative APIs that don&#x27;t use shell</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="129">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="130">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/elasticsearch_database/base.py:116</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="131">
                    <div class="rec-header">
                        <span class="rec-title">Network fetch combined with code execution<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/openai_functions/openapi.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="132">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/sql_database/query.py:33</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="133">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:66</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="134">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_with_sources/retrieval.py:52</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="135">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_generation/base.py:121</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="136">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_generation/base.py:116</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="137">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/natbot/base.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="138">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;user_input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:147</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="139">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:135</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="140">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="141">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:250</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="142">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM-generated code in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Code Execution Security:
1. NEVER execute LLM-generated code directly with exec()/eval()
2. If code execution is necessary, use sandboxed environments (Docker, VM)
3. Implement strict code validation and static analysis before execution
4. Use allowlists for permitted functions/modules
5. Set resource limits (CPU, memory, time) for execution
6. Parse and validate code structure before running
7. Consider using safer alternatives (JSON, declarative configs)
8. Log all code execution attempts with full context
9. Require human review for generated code
10. Use tools like RestrictedPython for safer Python execution

High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="143">
                    <div class="rec-header">
                        <span class="rec-title">Direct execution of LLM output in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:198</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">NEVER directly execute LLM-generated code:

1. Remove direct execution:
   - Do not use eval(), exec(), or os.system()
   - Avoid dynamic code execution
   - Use safer alternatives (allow-lists)

2. If code generation is required:
   - Generate code for review only
   - Require human approval before execution
   - Use sandboxing (containers, VMs)
   - Implement strict security policies

3. Use structured outputs:
   - Return data, not code
   - Use JSON schemas
   - Define clear interfaces

4. Add safeguards:
   - Static code analysis before execution
   - Whitelist allowed operations
   - Rate limiting and monitoring</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="144">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="145">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:96</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="146">
                    <div class="rec-header">
                        <span class="rec-title">LLM output used in dangerous sql_injection sink<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py:228</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations for SQL Injection:
1. Use parameterized queries: cursor.execute(query, (param,))
2. Never concatenate LLM output into SQL
3. Use ORM query builders (SQLAlchemy, Django ORM)
4. Apply strict input validation
5. Use read-only database connections where possible</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="147">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py:204</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="148">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/json_chat/base.py:14</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="149">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/react/agent.py:16</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="150">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="151">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:95</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="152">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:88</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="153">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py:31</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="154">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/chain_extract.py:68</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="155">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/self_query/base.py:316</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="156">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/self_query/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="157">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:402</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="158">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:492</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="159">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:389</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="160">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/chat_models.py:1113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="161">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake.py:106</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="162">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="163">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:378</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="164">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;inputs&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:431</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="165">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:518</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="166">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;prompts&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:788</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="167">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:368</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="168">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:508</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="169">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:781</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="170">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake_chat_models.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="171">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;query&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/retriever.py:65</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="172">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/retriever.py:62</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="173">
                    <div class="rec-header">
                        <span class="rec-title">Network fetch combined with code execution<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/graph_mermaid.py:44</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Remote Code Patterns:
1. NEVER execute code fetched from network without verification
2. Use cryptographic signatures to verify downloaded code
3. Pin URLs and verify checksums
4. Use package managers instead of direct downloads
5. Sandbox execution in isolated environments</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="174">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:189</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="175">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:185</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="176">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:142</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="177">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="178">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:215</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="179">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:234</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="180">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="181">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:327</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="182">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:189</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="183">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:296</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="184">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/retry.py:188</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="185">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/retry.py:179</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="186">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:162</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="187">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="188">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:107</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="189">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:136</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="190">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:153</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="191">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:979</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="192">
                    <div class="rec-header">
                        <span class="rec-title">User input &#x27;input_&#x27; embedded in LLM prompt<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:975</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Mitigations:
1. Use structured prompt templates (e.g., LangChain PromptTemplate)
2. Implement input sanitization to remove prompt injection patterns
3. Use separate &#x27;user&#x27; and &#x27;system&#x27; message roles (ChatML format)
4. Apply input validation and length limits
5. Use allowlists for expected input formats
6. Consider prompt injection detection libraries</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="193">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, LLM calls in loops, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:1130</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="194">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:5685</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="195">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:901</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-critical rec-hidden" data-priority="critical" data-index="196">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No input length validation, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:970</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-critical">critical</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="197">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1698</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="198">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1724</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="199">
                    <div class="rec-header">
                        <span class="rec-title">High-risk delete/write/network operation without confirmation in &#x27;_construct_responses_api_payload&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:3754</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="200">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;_generate&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1338</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="201">
                    <div class="rec-header">
                        <span class="rec-title">Code execution on external content<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:147</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Code Execution:
1. NEVER use eval/exec on untrusted input
2. Use safe alternatives (json.loads, ast.literal_eval)
3. Validate and sanitize all external content
4. Use sandboxed execution environments</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="202">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;_convert_messages_to_ollama_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:812</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="203">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/llms.py:356</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="204">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/huggingface/langchain_huggingface/chat_models/huggingface.py:538</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="205">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:410</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="206">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1881</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="207">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1004</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="208">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1019</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="209">
                    <div class="rec-header">
                        <span class="rec-title">High-risk delete/execute/network operation without confirmation in &#x27;_format_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:410</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="210">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/qdrant/langchain_qdrant/fastembed_sparse.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="211">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/qdrant/langchain_qdrant/fastembed_sparse.py:79</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="212">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:415</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="213">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute operation without confirmation in &#x27;_stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:415</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="214">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute operation without confirmation in &#x27;_generate&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/perplexity/langchain_perplexity/chat_models.py:589</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="215">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="216">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;bind_tools&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="217">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/prompty/langchain_prompty/core.py:191</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="218">
                    <div class="rec-header">
                        <span class="rec-title">Hardcoded Generic API Key detected in assignment<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/mistralai/langchain_mistralai/chat_models.py:120</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Remove hardcoded secrets immediately:
1. Use environment variables: os.getenv(&#x27;API_KEY&#x27;)
2. Use secret management: AWS Secrets Manager, Azure Key Vault, HashiCorp Vault
3. Use configuration files (never commit to git): config.ini, .env
4. Rotate the exposed secret immediately
5. Scan git history for leaked secrets: git-secrets, truffleHog
6. Add secret scanning to CI/CD pipeline</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="219">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/embeddings/base.py:90</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="220">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/embeddings/base.py:144</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="221">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1100</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="222">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_execute_model_sync&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1100</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="223">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:218</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="224">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;wrap_model_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:218</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="225">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;count_tokens&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:244</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="226">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;count_tokens&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/context_editing.py:281</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="227">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/contextual_compression.py:27</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="228">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/merger_retriever.py:53</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="229">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;merge_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/merger_retriever.py:53</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="230">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/re_phraser.py:61</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="231">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;rank_fusion&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/ensemble.py:202</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="232">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:164</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="233">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;generate_queries&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/multi_query.py:185</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="234">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/embeddings/base.py:28</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="235">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/embeddings/base.py:83</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="236">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/chat_memory.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="237">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/summary_buffer.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="238">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;prune&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/summary_buffer.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="239">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py:145</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="240">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;save_context&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/vectorstore_token_buffer_memory.py:145</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="241">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/entity.py:607</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="242">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/token_buffer.py:61</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="243">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;save_context&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/token_buffer.py:61</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="244">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:419</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="245">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:531</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="246">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:97</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="247">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/retry.py:234</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="248">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/fix.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="249">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;parse&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/output_parsers/fix.py:70</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="250">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/loading.py:115</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="251">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;load_evaluator&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/loading.py:115</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="252">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="253">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;apply&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="254">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;__call__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/base.py:369</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="255">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="256">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:104</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="257">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/hyde/base.py:89</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="258">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py:77</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="259">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:450</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="260">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:524</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="261">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:268</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="262">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;_get_docs&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/retrieval_qa/base.py:268</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="263">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_with_sources/retrieval.py:46</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="264">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_get_docs&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_with_sources/retrieval.py:46</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="265">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/qa_generation/base.py:116</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="266">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/natbot/base.py:113</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="267">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py:408</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="268">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_get_docs&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/conversational_retrieval/base.py:408</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="269">
                    <div class="rec-header">
                        <span class="rec-title">Code execution on external content<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:237</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Code Execution:
1. NEVER use eval/exec on untrusted input
2. Use safe alternatives (json.loads, ast.literal_eval)
3. Validate and sanitize all external content
4. Use sandboxed execution environments</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="270">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_do_generation&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:135</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="271">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/scoring/eval_chain.py:240</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="272">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/comparison/eval_chain.py:240</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="273">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py:97</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="274">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_tools/base.py:17</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="275">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, LLM calls in loops, No timeout configuration<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py:75</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="276">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;save_context&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/agent_token_buffer_memory.py:75</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="277">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="278">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="279">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/xml/base.py:115</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="280">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="281">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:90</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="282">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:589</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="283">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_get_response&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_assistant/base.py:589</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="284">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:929</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="285">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1512</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="286">
                    <div class="rec-header">
                        <span class="rec-title">Insecure tool function &#x27;_run_llm&#x27; executes dangerous operations<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool/Plugin Implementation:
1. NEVER execute shell commands from LLM output directly
2. Use allowlists for permitted commands/operations
3. Validate all file paths against allowed directories
4. Use parameterized queries - never raw SQL from LLM
5. Validate URLs against allowlist before HTTP requests
6. Implement strict input schemas (JSON Schema, Pydantic)
7. Add rate limiting and request throttling
8. Log all tool invocations for audit
9. Use principle of least privilege
10. Implement human-in-the-loop for destructive operations</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="287">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_run_chain&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:929</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="288">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/listwise_rerank.py:102</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="289">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;compress_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/document_compressors/cross_encoder_rerank.py:31</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="290">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;_get_relevant_documents&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/retrievers/self_query/base.py:310</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="291">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake.py:98</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="292">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/llms.py:415</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="293">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/language_models/fake_chat_models.py:158</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="294">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="295">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="296">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:253</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="297">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:450</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="298">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:279</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="299">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;tool&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:76</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="300">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;convert_runnable_to_tool&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:413</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="301">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute/network operation without confirmation in &#x27;_create_tool_factory&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:253</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="302">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;invoke_wrapper&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:450</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="303">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute operation without confirmation in &#x27;invoke_wrapper&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/convert.py:279</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="304">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tools/retriever.py:31</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="305">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/pydantic.py:231</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="306">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/function_calling.py:193</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="307">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="308">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:142</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="309">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:156</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="310">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/configurable.py:178</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="311">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:189</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="312">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/branch.py:296</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="313">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/retry.py:179</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="314">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:107</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="315">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:136</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="316">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/router.py:153</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="317">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/passthrough.py:480</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="318">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/passthrough.py:480</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="319">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:867</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="320">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:937</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="321">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:4847</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="322">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;batch&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:867</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="323">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;batch_as_completed&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:937</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="324">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:1130</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="325">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;_invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:4847</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="326">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:5685</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="327">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:901</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="328">
                    <div class="rec-header">
                        <span class="rec-title">High-risk execute/network operation without confirmation in &#x27;invoke&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/runnables/base.py:970</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="329">
                    <div class="rec-header">
                        <span class="rec-title">Model DoS vulnerability: No rate limiting, No timeout configuration, No token/context limits<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tracers/evaluation.py:64</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">Model DoS Mitigations:
1. Implement rate limiting per user/IP (@limiter.limit(&#x27;10/minute&#x27;))
2. Validate and limit input length (max 1000 chars)
3. Set token limits (max_tokens=500)
4. Configure timeouts (timeout=30 seconds)
5. Avoid LLM calls in unbounded loops
6. Implement circuit breakers for cascading failures
7. Monitor and alert on resource usage
8. Use queuing for batch processing
9. Implement cost controls and budgets</p>
                </div>
                
                <div class="rec-item rec-high rec-hidden" data-priority="high" data-index="330">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write/execute operation without confirmation in &#x27;__init__&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/tracers/evaluation.py:64</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-high">high</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="331">
                    <div class="rec-header">
                        <span class="rec-title">High-risk network operation without confirmation in &#x27;_tokenize&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/embeddings/base.py:429</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="332">
                    <div class="rec-header">
                        <span class="rec-title">High-risk network operation without confirmation in &#x27;get_num_tokens_from_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1724</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="333">
                    <div class="rec-header">
                        <span class="rec-title">High-risk network operation without confirmation in &#x27;_chat_params&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:720</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="334">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write operation without confirmation in &#x27;_create_chat_stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:945</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="335">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1885</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="336">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;_format_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:489</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="337">
                    <div class="rec-header">
                        <span class="rec-title">High-risk network operation without confirmation in &#x27;convert_to_anthropic_tool&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1792</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="338">
                    <div class="rec-header">
                        <span class="rec-title">High-risk network operation without confirmation in &#x27;_lc_tool_calls_to_anthropic_tool_use_blocks&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:1881</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="339">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;bind_tools&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:422</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="340">
                    <div class="rec-header">
                        <span class="rec-title">High-risk network operation without confirmation in &#x27;save_context&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/chat_memory.py:74</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="341">
                    <div class="rec-header">
                        <span class="rec-title">Dynamic tool/plugin loading<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/__init__.py:56</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="342">
                    <div class="rec-header">
                        <span class="rec-title">Dynamic tool/plugin loading<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/__init__.py:154</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="343">
                    <div class="rec-header">
                        <span class="rec-title">Unpinned model version in API call<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/evaluation/loading.py:168</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Supply Chain Security Best Practices:
1. Pin model versions explicitly (model=&#x27;gpt-4-0613&#x27;)
2. Use model registries with version control
3. Document model versions in requirements.txt or similar
4. Implement model versioning in CI/CD pipelines</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="344">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;create_openai_tools_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_tools/base.py:100</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="345">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;create_openai_functions_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:372</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="346">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:125</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="347">
                    <div class="rec-header">
                        <span class="rec-title">LLM tool calling without permission checks in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_multi_agent/base.py:228</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Tool Calling Security Best Practices:
1. Implement permission checks before tool execution (check_permission, authorize)
2. Use allowlists to restrict which tools can be called
3. Require human confirmation for sensitive operations
4. Log all tool executions with context for audit trails
5. Implement rate limiting on tool calls to prevent abuse
6. Use least-privilege principle - only grant necessary permissions
7. Add input validation for tool parameters
8. Consider implementing a &quot;dry-run&quot; mode for testing
9. Set up alerts for unusual tool usage patterns
10. Document tool permissions and restrictions clearly</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="348">
                    <div class="rec-header">
                        <span class="rec-title">Dynamic tool/plugin loading<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="349">
                    <div class="rec-header">
                        <span class="rec-title">Dynamic tool/plugin loading<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:44</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="350">
                    <div class="rec-header">
                        <span class="rec-title">Dynamic tool/plugin loading<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:67</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Tool Loading:
1. Validate tool names against an allowlist
2. Only load tools from trusted sources
3. Review tool permissions before loading
4. Sandbox tool execution</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="351">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write operation without confirmation in &#x27;_create_subset_model_v2&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/pydantic.py:231</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="352">
                    <div class="rec-header">
                        <span class="rec-title">High-risk write operation without confirmation in &#x27;_convert_python_function_to_openai_function&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/function_calling.py:193</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">High-Risk Operation Safety:
1. Require explicit user confirmation for destructive actions
2. Display clear preview of what will be changed/deleted
3. Implement &quot;undo&quot; functionality where possible
4. Use transaction rollback for database operations
5. Add time delays before executing irreversible actions
6. Send notifications for critical operations
7. Implement approval workflows for sensitive operations
8. Maintain detailed audit logs of all actions
9. Use &quot;dry-run&quot; mode to show what would happen
10. Consider implementing operation quotas/limits</p>
                </div>
                
                <div class="rec-item rec-medium rec-hidden" data-priority="medium" data-index="353">
                    <div class="rec-header">
                        <span class="rec-title">Code execution on external content<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/documents/base.py:7</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-medium">medium</span>
                        </div>
                    </div>
                    <p class="rec-description">Secure Code Execution:
1. NEVER use eval/exec on untrusted input
2. Use safe alternatives (json.loads, ast.literal_eval)
3. Validate and sanitize all external content
4. Use sandboxed execution environments</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="354">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_construct_responses_api_payload&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:3754</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="355">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;get_num_tokens_from_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/openai/langchain_openai/chat_models/base.py:1724</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="356">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_convert_messages_to_ollama_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:812</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="357">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;_create_chat_stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/chat_models.py:945</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="358">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;_create_generate_stream&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/ollama/langchain_ollama/llms.py:356</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="359">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_format_messages&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/anthropic/langchain_anthropic/chat_models.py:410</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="360">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;bind_tools&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/partners/deepseek/langchain_deepseek/chat_models.py:395</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="361">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;_execute_model_sync&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/factory.py:1100</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="362">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;wrap_model_call&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/tool_selection.py:270</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="363">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;_create_summary&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain_v1/langchain/agents/middleware/summarization.py:562</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="364">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;prune&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/memory/summary_buffer.py:112</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="365">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:419</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="366">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;plan&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/agent.py:531</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="367">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;apply&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/llm.py:224</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="368">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;query&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:34</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="369">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;query_with_sources&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/indexes/vectorstore.py:104</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="370">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;create_citation_fuzzy_match_runnable&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/openai_functions/citation_fuzzy_match.py:77</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="371">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_sql_query_chain&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/sql_database/query.py:33</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="372">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;_create_openai_tools_runnable&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/structured_output/base.py:450</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="373">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;from_llm&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/chains/flare/base.py:250</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="374">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;create_self_ask_with_search_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/self_ask_with_search/base.py:97</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="375">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_openai_tools_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_tools/base.py:17</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="376">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_openai_functions_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/openai_functions_agent/base.py:287</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="377">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_tool_calling_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/tool_calling_agent/base.py:18</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="378">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_structured_chat_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/structured_chat/base.py:166</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="379">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_json_chat_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/json_chat/base.py:14</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="380">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_xml_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/xml/base.py:115</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="381">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;create_react_agent&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/agents/react/agent.py:16</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="382">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_run_llm&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:861</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="383">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;run_on_dataset&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:1512</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical security, data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards

Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="384">
                    <div class="rec-header">
                        <span class="rec-title">Automated action without confidence threshold in &#x27;_run_chain&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/langchain/langchain_classic/smith/evaluation/runner_utils.py:929</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Implement confidence thresholds for automated actions:

1. Add confidence scoring:
   - Request confidence scores from LLM
   - Calculate custom confidence metrics
   - Track historical accuracy

2. Set thresholds:
   - High confidence (&gt;0.9): Auto-execute
   - Medium confidence (0.7-0.9): Human review
   - Low confidence (&lt;0.7): Reject or escalate

3. Validate output:
   - Use schema validation (Pydantic)
   - Check output format and constraints
   - Verify against expected patterns

4. Implement fallbacks:
   - Have backup strategies for low confidence
   - Use simpler/safer alternatives
   - Escalate to human operators</p>
                </div>
                
                <div class="rec-item rec-low rec-hidden" data-priority="low" data-index="385">
                    <div class="rec-header">
                        <span class="rec-title">Critical decision without oversight in &#x27;_convert_pydantic_to_openai_function&#x27;<span style="color: #64748b; font-size: 0.8em; margin-left: 8px;">/var/folders/xr/487hfwq1609_955mpzlxvkb80000gn/T/aisentry-scan-alrxscxf/libs/core/langchain_core/utils/function_calling.py:153</span></span>
                        <div style="display: flex; gap: 6px;">
                            <span class="rec-priority" style="background: #f97316; color: white;">Scan</span>
                            <span class="rec-priority priority-low">low</span>
                        </div>
                    </div>
                    <p class="rec-description">Critical data_modification decision requires human oversight:

1. Implement human-in-the-loop review:
   - Add review queue for high-stakes decisions
   - Require explicit human approval before execution
   - Log all decisions for audit trail

2. Add verification mechanisms:
   - Cross-reference with trusted sources
   - Implement multi-step verification
   - Use confidence thresholds

3. Include safety checks:
   - Set limits on transaction amounts
   - Require secondary confirmation
   - Implement rollback mechanisms

4. Add disclaimers:
   - Inform users output may be incorrect
   - Recommend professional consultation
   - Document limitations clearly

5. Monitor and review:
   - Track decision outcomes
   - Review failures and near-misses
   - Continuously improve safeguards</p>
                </div>
                
                </div>

                <!-- Load More Button -->
                <div id="load-more-container" style="text-align: center; margin-top: 16px;">
                    <button id="load-more-btn" class="load-more-btn" onclick="loadMoreRecommendations()">
                        Show More <span id="remaining-count"></span>
                    </button>
                </div>
            </div>
            
            </div>
            
        <footer>
            <p>Generated by <strong>aisentry CLI</strong> | <a href="https://aisentry.co" target="_blank">aisentry.co</a></p>
        </footer>
    </div>
    <script>
        
        // Filter state
        const filterState = {
            severities: new Set(['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']),
            categories: new Set(),
            searchText: ''
        };

        // Initialize on DOM ready
        document.addEventListener('DOMContentLoaded', initializeFilters);

        function initializeFilters() {
            const findings = document.querySelectorAll('.finding, .vulnerability');
            if (findings.length === 0) return;

            // Collect unique categories
            const categories = new Set();
            findings.forEach(f => {
                if (f.dataset.category) {
                    categories.add(f.dataset.category);
                    filterState.categories.add(f.dataset.category);
                }
            });

            // Build severity chips
            const severityContainer = document.getElementById('severity-chips');
            if (severityContainer) {
                ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO'].forEach(sev => {
                    const count = document.querySelectorAll('[data-severity="' + sev + '"]').length;
                    if (count > 0) {
                        const chip = document.createElement('span');
                        chip.className = 'filter-chip severity-chip active';
                        chip.dataset.type = 'severity';
                        chip.dataset.value = sev;
                        chip.innerHTML = sev + ' <span class="chip-count">(' + count + ')</span>';
                        chip.onclick = function() { toggleChip(this); };
                        severityContainer.appendChild(chip);
                    }
                });
            }

            // Build category chips
            const categoryContainer = document.getElementById('category-chips');
            if (categoryContainer && categories.size > 0) {
                // Sort categories
                const sortedCats = Array.from(categories).sort();
                sortedCats.forEach(cat => {
                    const count = document.querySelectorAll('[data-category="' + cat + '"]').length;
                    const chip = document.createElement('span');
                    chip.className = 'filter-chip category-chip active';
                    chip.dataset.type = 'category';
                    chip.dataset.value = cat;
                    // Shorten category name for display
                    const shortName = cat.length > 20 ? cat.substring(0, 20) + '...' : cat;
                    chip.innerHTML = shortName + ' <span class="chip-count">(' + count + ')</span>';
                    chip.title = cat;
                    chip.onclick = function() { toggleChip(this); };
                    categoryContainer.appendChild(chip);
                });
            }

            // Search input handler
            const searchInput = document.getElementById('filter-search');
            if (searchInput) {
                searchInput.oninput = function() {
                    filterState.searchText = this.value.toLowerCase();
                    applyFilters();
                };
            }

            updateStats();
        }

        function toggleChip(chip) {
            const type = chip.dataset.type;
            const value = chip.dataset.value;

            if (chip.classList.contains('active')) {
                chip.classList.remove('active');
                if (type === 'severity') {
                    filterState.severities.delete(value);
                } else {
                    filterState.categories.delete(value);
                }
            } else {
                chip.classList.add('active');
                if (type === 'severity') {
                    filterState.severities.add(value);
                } else {
                    filterState.categories.add(value);
                }
            }
            applyFilters();
        }

        function applyFilters() {
            const findings = document.querySelectorAll('.finding, .vulnerability');
            let visible = 0;

            findings.forEach(f => {
                const sev = f.dataset.severity;
                const cat = f.dataset.category;
                const text = f.textContent.toLowerCase();

                const sevMatch = filterState.severities.has(sev);
                const catMatch = filterState.categories.size === 0 || filterState.categories.has(cat);
                const searchMatch = !filterState.searchText || text.includes(filterState.searchText);

                if (sevMatch && catMatch && searchMatch) {
                    f.classList.remove('filtered-out');
                    visible++;
                } else {
                    f.classList.add('filtered-out');
                }
            });

            updateStats();

            // Show/hide no results
            const noResults = document.getElementById('no-results');
            if (noResults) {
                noResults.style.display = visible === 0 ? 'block' : 'none';
            }
        }

        function updateStats() {
            const total = document.querySelectorAll('.finding, .vulnerability').length;
            const visible = document.querySelectorAll('.finding:not(.filtered-out), .vulnerability:not(.filtered-out)').length;
            const statsEl = document.getElementById('filter-stats');
            if (statsEl) {
                statsEl.innerHTML = 'Showing <strong>' + visible + '</strong> of <strong>' + total + '</strong>';
            }
        }

        function selectAllSeverity() {
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.severities.add(chip.dataset.value);
            });
            applyFilters();
        }

        function selectNoneSeverity() {
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.remove('active');
                filterState.severities.delete(chip.dataset.value);
            });
            applyFilters();
        }

        function selectAllCategory() {
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.categories.add(chip.dataset.value);
            });
            applyFilters();
        }

        function selectNoneCategory() {
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.remove('active');
                filterState.categories.delete(chip.dataset.value);
            });
            applyFilters();
        }

        function resetFilters() {
            // Reset all severity chips
            document.querySelectorAll('.filter-chip.severity-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.severities.add(chip.dataset.value);
            });

            // Reset all category chips
            document.querySelectorAll('.filter-chip.category-chip').forEach(chip => {
                chip.classList.add('active');
                filterState.categories.add(chip.dataset.value);
            });

            // Reset search
            const searchInput = document.getElementById('filter-search');
            if (searchInput) {
                searchInput.value = '';
                filterState.searchText = '';
            }

            applyFilters();
        }

        // Tab navigation
        function switchTab(tabId) {
            // Update tab buttons
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.tab === tabId) {
                    btn.classList.add('active');
                }
            });

            // Update tab content
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
                if (content.id === tabId) {
                    content.classList.add('active');
                }
            });

            // Re-initialize filters for the active tab if needed
            setTimeout(initializeFilters, 100);
        }

        // Dark mode toggle
        function toggleTheme() {
            const html = document.documentElement;
            const currentTheme = html.getAttribute('data-theme');
            const newTheme = currentTheme === 'light' ? 'dark' : 'light';
            html.setAttribute('data-theme', newTheme);
            localStorage.setItem('theme', newTheme);
        }

        // Initialize theme from localStorage
        function initTheme() {
            const savedTheme = localStorage.getItem('theme');
            if (savedTheme) {
                document.documentElement.setAttribute('data-theme', savedTheme);
            } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                document.documentElement.setAttribute('data-theme', 'dark');
            }
        }

        // Initialize tabs on load
        document.addEventListener('DOMContentLoaded', function() {
            // Initialize theme
            initTheme();

            // Set up tab click handlers
            document.querySelectorAll('.tab-btn').forEach(btn => {
                btn.onclick = function() {
                    switchTab(this.dataset.tab);
                };
            });

            // Initialize recommendation pagination
            initRecommendations();
            // Initialize findings pagination
            initFindings();
        });

        // Recommendation pagination state
        let recState = {
            visibleCount: 10,
            pageSize: 10,
            currentFilter: 'all',
            totalItems: 0
        };

        function initRecommendations() {
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            recState.totalItems = items.length;
            updateLoadMoreButton();
        }

        function updateLoadMoreButton() {
            const btn = document.getElementById('load-more-btn');
            const container = document.getElementById('load-more-container');
            const countSpan = document.getElementById('remaining-count');

            if (!btn || !container) return;

            // Count visible items based on current filter
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let matchingItems = 0;
            let visibleItems = 0;

            items.forEach(item => {
                const priority = item.dataset.priority;
                const matchesFilter = recState.currentFilter === 'all' || priority === recState.currentFilter;

                if (matchesFilter) {
                    matchingItems++;
                    if (!item.classList.contains('rec-hidden')) {
                        visibleItems++;
                    }
                }
            });

            const remaining = matchingItems - visibleItems;

            if (remaining <= 0) {
                container.style.display = 'none';
            } else {
                container.style.display = 'block';
                countSpan.textContent = `(${remaining} more)`;
            }
        }

        function loadMoreRecommendations() {
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let shown = 0;
            let toShow = recState.pageSize;

            items.forEach(item => {
                const priority = item.dataset.priority;
                const matchesFilter = recState.currentFilter === 'all' || priority === recState.currentFilter;

                if (matchesFilter && item.classList.contains('rec-hidden') && toShow > 0) {
                    item.classList.remove('rec-hidden');
                    toShow--;
                }
            });

            updateLoadMoreButton();
        }

        function filterBySeverity(severity) {
            recState.currentFilter = severity;

            // Update button states
            document.querySelectorAll('.severity-filter-btn').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.severity === severity) {
                    btn.classList.add('active');
                }
            });

            // Filter items
            const items = document.querySelectorAll('#recommendations-list .rec-item');
            let visibleCount = 0;

            items.forEach((item, index) => {
                const priority = item.dataset.priority;
                const matchesFilter = severity === 'all' || priority === severity;

                // Remove all visibility classes first
                item.classList.remove('rec-hidden', 'rec-filtered');

                if (!matchesFilter) {
                    item.classList.add('rec-filtered');
                } else {
                    // Show first 10 matching items
                    if (visibleCount >= recState.pageSize) {
                        item.classList.add('rec-hidden');
                    }
                    visibleCount++;
                }
            });

            updateLoadMoreButton();
        }

        // Findings pagination state
        let findingsState = {
            visibleCount: 10,
            pageSize: 10,
            currentFilter: 'all',
            totalItems: 0
        };

        function initFindings() {
            const items = document.querySelectorAll('#findings-list .finding');
            findingsState.totalItems = items.length;
            updateFindingsLoadMoreButton();
        }

        function updateFindingsLoadMoreButton() {
            const btn = document.getElementById('findings-load-more-btn');
            const container = document.getElementById('findings-load-more-container');
            const countSpan = document.getElementById('findings-remaining-count');

            if (!btn || !container) return;

            const items = document.querySelectorAll('#findings-list .finding');
            let matchingItems = 0;
            let visibleItems = 0;

            items.forEach(item => {
                const severity = item.dataset.severity;
                const matchesFilter = findingsState.currentFilter === 'all' || severity === findingsState.currentFilter;

                if (matchesFilter) {
                    matchingItems++;
                    if (!item.classList.contains('finding-hidden')) {
                        visibleItems++;
                    }
                }
            });

            const remaining = matchingItems - visibleItems;

            if (remaining <= 0) {
                container.style.display = 'none';
            } else {
                container.style.display = 'block';
                countSpan.textContent = `(${remaining} more)`;
            }
        }

        function loadMoreFindings() {
            const items = document.querySelectorAll('#findings-list .finding');
            let toShow = findingsState.pageSize;

            items.forEach(item => {
                const severity = item.dataset.severity;
                const matchesFilter = findingsState.currentFilter === 'all' || severity === findingsState.currentFilter;

                if (matchesFilter && item.classList.contains('finding-hidden') && toShow > 0) {
                    item.classList.remove('finding-hidden');
                    toShow--;
                }
            });

            updateFindingsLoadMoreButton();
        }

        function filterFindingsBySeverity(severity) {
            findingsState.currentFilter = severity;

            // Update button states (only for findings buttons)
            document.querySelectorAll('.severity-filter-btn[data-target="findings"]').forEach(btn => {
                btn.classList.remove('active');
                if (btn.dataset.severity === severity) {
                    btn.classList.add('active');
                }
            });

            // Filter items
            const items = document.querySelectorAll('#findings-list .finding');
            let visibleCount = 0;

            items.forEach((item, index) => {
                const itemSeverity = item.dataset.severity;
                const matchesFilter = severity === 'all' || itemSeverity === severity;

                // Remove all visibility classes first
                item.classList.remove('finding-hidden', 'finding-filtered');

                if (!matchesFilter) {
                    item.classList.add('finding-filtered');
                } else {
                    // Show first 10 matching items
                    if (visibleCount >= findingsState.pageSize) {
                        item.classList.add('finding-hidden');
                    }
                    visibleCount++;
                }
            });

            updateFindingsLoadMoreButton();
        }

        // Category accordion functionality
        function toggleCategory(header) {
            const card = header.parentElement;
            card.classList.toggle('open');
        }

        function toggleAllCategories() {
            const cards = document.querySelectorAll('.category-card');
            const btn = document.getElementById('toggle-categories');
            const allOpen = [...cards].every(card => card.classList.contains('open'));

            cards.forEach(card => {
                if (allOpen) {
                    card.classList.remove('open');
                } else {
                    card.classList.add('open');
                }
            });

            if (btn) {
                btn.textContent = allOpen ? 'Expand All' : 'Collapse All';
            }
        }
        
    </script>
</body>
</html>